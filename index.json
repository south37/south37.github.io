[{"content":"はじめに mold と呼ばれる高速なリンカを利用して Chromium を Build してみる という記事の中で、mold と呼ばれる「高速なリンカ」について紹介しました。\nhttps://github.com/rui314/mold\nmold は、自分の知る限りでは現時点では特に Binary の配信などは行っていないようです。利用したい場合には repository を git clone して、自分で Build して利用する必要があります。\nmold の Build 手順についてメモ程度に記録を残しておこうと思います。\n2021年3月20日追記: ちょうど4日ほど前に mold の README に How to build というセクションが追加されたようです。最新のソースコードで Build する場合は、そちらを参照してみてください。cf. https://github.com/rui314/mold#how-to-build\nステップ1. mold のソースコードを取得する まず、mold の git repository を clone します。\nminami@chromium-dev-20210227:~$ git clone https://github.com/rui314/mold.git minami@chromium-dev-20210227:~$ ls mold minami@chromium-dev-20210227:~$ cd mold/ mold は自身が依存する mimalloc と oneTBB を git submodule として利用しているので、git submodule update --init を実行してこれらのソースコードを取得します。\nminami@chromium-dev-20210227:~/mold$ git submodule update --init Submodule \u0026#39;mimalloc\u0026#39; (https://github.com/microsoft/mimalloc.git) registered for path \u0026#39;mimalloc\u0026#39; Submodule \u0026#39;oneTBB\u0026#39; (https://github.com/oneapi-src/oneTBB.git) registered for path \u0026#39;oneTBB\u0026#39; Cloning into \u0026#39;/home/minami/mold/mimalloc\u0026#39;... Cloning into \u0026#39;/home/minami/mold/oneTBB\u0026#39;... Submodule path \u0026#39;mimalloc\u0026#39;: checked out \u0026#39;4cc8bff90d9e081298ca2c1a94024c7ad4a9e478\u0026#39; Submodule path \u0026#39;oneTBB\u0026#39;: checked out \u0026#39;eca91f16d7490a8abfdee652dadf457ec820cc37\u0026#39; これで、必要なソースコードの取得は完了しました。\nステップ2. 必要な package を install して Build する make, libssl-dev, zlib1g-dev, cmake, build-essential を利用するので、apt で install しておきます（cmake, build-essential は git submodule で取り込んだ mimalloc と oneTBB の Build に利用します）。\nminami@chromium-dev-20210227:~/mold$ sudo apt update minami@chromium-dev-20210227:~/mold$ sudo apt install -y make libssl-dev zlib1g-dev cmake build-essential $ make submodules で、submodule で取り込んだ oneTBB と mimalloc を Build します。\nminami@chromium-dev-20210227:~/mold$ make submodules make -C oneTBB make[1]: Entering directory \u0026#39;/home/minami/mold/oneTBB\u0026#39; Created the ./build/linux_intel64_gcc_cc9.3.0_libc2.31_kernel5.4.0_release directory make -C \u0026#34;./build/linux_intel64_gcc_cc9.3.0_libc2.31_kernel5.4.0_release\u0026#34; -r -f ../../build/Makefile.tbb cfg=release make[2]: Entering directory \u0026#39;/home/minami/mold/oneTBB/build/linux_intel64_gcc_cc9.3.0_libc2.31_kernel5.4.0_release\u0026#39; . . . make[2]: Leaving directory \u0026#39;/home/minami/mold/oneTBB/build/linux_intel64_gcc_cc9.3.0_libc2.31_kernel5.4.0_release\u0026#39; make[1]: Leaving directory \u0026#39;/home/minami/mold/oneTBB\u0026#39; mkdir -p mimalloc/out/release (cd mimalloc/out/release; cmake ../..) . . . [100%] Built target mimalloc-test-api make[2]: Leaving directory \u0026#39;/home/minami/mold/mimalloc/out/release\u0026#39; make[1]: Leaving directory \u0026#39;/home/minami/mold/mimalloc/out/release\u0026#39; これで、oneTBB と mimalloc の Build は完了しました。\nいよいよ、mold の Build を行いたいと思いいます。ただし、この状態では clang++ が無いのでまだ Build できません。\nminami@chromium-dev-20210227:~/mold$ make clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o main.o main.cc make: clang++: Command not found make: *** [\u0026lt;builtin\u0026gt;: main.o] Error 127 clang++ の install では、https://apt.llvm.org/ の「Automatic installation script」である bash -c \u0026quot;$(wget -O - https://apt.llvm.org/llvm.sh)\u0026quot; を利用する事にします。\nminami@chromium-dev-20210227:~/mold$ sudo bash -c \u0026#34;$(wget -O - https://apt.llvm.org/llvm.sh)\u0026#34; --2021-02-27 00:50:07-- https://apt.llvm.org/llvm.sh . . . Processing triggers for install-info (6.7.0.dfsg.2-5) ... Processing triggers for libc-bin (2.31-0ubuntu9.2) ... これで、clang++-11 が install されます（注: 2021/02/27 時点の話で、時期によって最新 version は違うかもしれません）。 clang++ として利用できるように、symlink を貼っておきます。\nminami@chromium-dev-20210227:~/mold$ ls /usr/bin | grep clang clang++-11 clang-11 clang-cpp-11 clangd-11 minami@chromium-dev-20210227:~/mold$ sudo ln -s /usr/bin/clang++-11 /usr/bin/clang++ これで clang++ は使えるようになりましたが、まだ「span header が見つからない」というエラーが出る状態です。\nminami@chromium-dev-20210227:~/mold$ make clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o main.o main.cc In file included from main.cc:1: ./mold.h:17:10: fatal error: \u0026#39;span\u0026#39; file not found #include \u0026lt;span\u0026gt; ^~~~~~ 1 error generated. make: *** [\u0026lt;builtin\u0026gt;: main.o] Error 1 make で実行されているコマンドに -v オプションをつけると詳細が表示されるのですが、この include path の中で span が見つからないのが原因のようです。\nminami@chromium-dev-20210227:~/mold$ clang++ -v -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o main.o main.cc Ubuntu clang version 11.1.0-++20210204121720+1fdec59bffc1-1~exp1~20210203232336.162 Target: x86_64-pc-linux-gnu Thread model: posix InstalledDir: /usr/bin Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/9 Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/9 Selected GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/9 Candidate multilib: .;@m64 Selected multilib: .;@m64 (in-process) \u0026#34;/usr/lib/llvm-11/bin/clang\u0026#34; -cc1 -triple x86_64-pc-linux-gnu -emit-obj -disable-free -disable-llvm-verifier -discard-value-names -main-file-name main.cc -mrelocation-model static -mframe-pointer=none -fmath-errno -fno-rounding-math -mconstructor-aliases -munwind-tables -target-cpu x86-64 -fno-split-dwarf-inlining -debug-info-kind=limited -dwarf-version=4 -debugger-tuning=gdb -v -resource-dir /usr/lib/llvm-11/lib/clang/11.1.0 -I oneTBB/include -internal-isystem /usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9 -internal-isystem /usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/x86_64-linux-gnu/c++/9 -internal-isystem /usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/x86_64-linux-gnu/c++/9 -internal-isystem /usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9/backward -internal-isystem /usr/local/include -internal-isystem /usr/lib/llvm-11/lib/clang/11.1.0/include -internal-externc-isystem /usr/include/x86_64-linux-gnu -internal-externc-isystem /include -internal-externc-isystem /usr/include -O2 -Wno-deprecated-volatile -Wno-switch -std=c++20 -fdeprecated-macro -fdebug-compilation-dir /home/minami/mold -ferror-limit 19 -pthread -fgnuc-version=4.2.1 -fcxx-exceptions -fexceptions -fcolor-diagnostics -vectorize-loops -vectorize-slp -faddrsig -o main.o -x c++ main.cc clang -cc1 version 11.1.0 based upon LLVM 11.1.0 default target x86_64-pc-linux-gnu ignoring nonexistent directory \u0026#34;oneTBB/include\u0026#34; ignoring nonexistent directory \u0026#34;/include\u0026#34; ignoring duplicate directory \u0026#34;/usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/x86_64-linux-gnu/c++/9\u0026#34; #include \u0026#34;...\u0026#34; search starts here: #include \u0026lt;...\u0026gt; search starts here: /usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9 /usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/x86_64-linux-gnu/c++/9 /usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9/backward /usr/local/include /usr/lib/llvm-11/lib/clang/11.1.0/include /usr/include/x86_64-linux-gnu /usr/include End of search list. In file included from main.cc:1: ./mold.h:17:10: fatal error: \u0026#39;span\u0026#39; file not found #include \u0026lt;span\u0026gt; ^~~~~~ 1 error generated. ここで、おもむろに libstdc++-10-dev package の install をします。\nminami@chromium-dev-20210227:~/mold$ sudo apt install -y libstdc++-10-dev 上記 package を install すると、ちゃんと span header を見つけることができます。\nminami@chromium-dev-20210227:~/mold$ clang++ -v -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o main.o main.cc Ubuntu clang version 11.1.0-++20210204121720+1fdec59bffc1-1~exp1~20210203232336.162 Target: x86_64-pc-linux-gnu Thread model: posix InstalledDir: /usr/bin Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/10 Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/9 Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/10 Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/9 Selected GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/10 Candidate multilib: .;@m64 Selected multilib: .;@m64 (in-process) \u0026#34;/usr/lib/llvm-11/bin/clang\u0026#34; -cc1 -triple x86_64-pc-linux-gnu -emit-obj -disable-free -disable-llvm-verifier -discard-value-names -main-file-name main.cc -mrelocation-model static -mframe-pointer=none -fmath-errno -fno-rounding-math -mconstructor-aliases -munwind-tables -target-cpu x86-64 -fno-split-dwarf-inlining -debug-info-kind=limited -dwarf-version=4 -debugger-tuning=gdb -v -resource-dir /usr/lib/llvm-11/lib/clang/11.1.0 -I oneTBB/include -internal-isystem /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10 -internal-isystem /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/x86_64-linux-gnu/c++/10 -internal-isystem /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/x86_64-linux-gnu/c++/10 -internal-isystem /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/backward -internal-isystem /usr/local/include -internal-isystem /usr/lib/llvm-11/lib/clang/11.1.0/include -internal-externc-isystem /usr/include/x86_64-linux-gnu -internal-externc-isystem /include -internal-externc-isystem /usr/include -O2 -Wno-deprecated-volatile -Wno-switch -std=c++20 -fdeprecated-macro -fdebug-compilation-dir /home/minami/mold -ferror-limit 19 -pthread -fgnuc-version=4.2.1 -fcxx-exceptions -fexceptions -fcolor-diagnostics -vectorize-loops -vectorize-slp -faddrsig -o main.o -x c++ main.cc clang -cc1 version 11.1.0 based upon LLVM 11.1.0 default target x86_64-pc-linux-gnu ignoring nonexistent directory \u0026#34;/include\u0026#34; ignoring duplicate directory \u0026#34;/usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/x86_64-linux-gnu/c++/10\u0026#34; #include \u0026#34;...\u0026#34; search starts here: #include \u0026lt;...\u0026gt; search starts here: oneTBB/include /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10 /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/x86_64-linux-gnu/c++/10 /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/backward /usr/local/include /usr/lib/llvm-11/lib/clang/11.1.0/include /usr/include/x86_64-linux-gnu /usr/include End of search list. 少し探してみると /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10 の中に span header があるのを見つけました。これが重要だったようです。\nminami@chromium-dev-20210227:~/mold$ ls -la /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/span -rw-r--r-- 1 root root 13251 Aug 8 2020 /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/span この状態で make を実行すると、mold の Build が行われて、mold Binary が生成されるはずです。\nminami@chromium-dev-20210227:~/mold$ make clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o output_chunks.o output_chunks.cc clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o mapfile.o mapfile.cc clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o perf.o perf.cc clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o linker_script.o linker_script.cc clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o archive_file.o archive_file.cc clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o output_file.o output_file.cc clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o subprocess.o subprocess.cc clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o gc_sections.o gc_sections.cc clang++ -g -IoneTBB/include -pthread -std=c++20 -Wno-deprecated-volatile -Wno-switch -O2 -c -o icf.o icf.cc clang++ main.o object_file.o input_sections.o output_chunks.o mapfile.o perf.o linker_script.o archive_file.o output_file.o subprocess.o gc_sections.o icf.o -o mold -L/home/minami/mold/oneTBB/build/linux_intel64_gcc_cc9.3.0_libc2.31_kernel5.4.0_release/ -Wl,-rpath=/home/minami/mold/oneTBB/build/linux_intel64_gcc_cc9.3.0_libc2.31_kernel5.4.0_release/ -L/home/minami/mold/mimalloc/out/release -Wl,-rpath=/home/minami/mold/mimalloc/out/release -lcrypto -pthread -ltbb -lmimalloc 以下のように mold Binary が生成されていれば成功です 🎉\nminami@chromium-dev-20210227:~/mold$ ls -la mold -rwxrwxr-x 1 minami minami 11142376 Feb 27 01:43 mold まとめ 「高速なリンカである mold」について、Build 手順をまとめました。\n今後、mold が広く使われるようになり、package での配信などが行われるようになればここに記載した手順はおそらく不要になると思います。しかしながら、mold はまだ開始したばかりの project であり、開発環境なども未整備の状態です。しばらくは、「自分で Build して動かしてみる」という状態が続くでしょう。\nこのブログが、「試しに mold を利用してみる」ことへの一助となれば幸いです。\n","permalink":"https://south37.link/posts/20210228-build-mold/","summary":"はじめに mold と呼ばれる高速なリンカを利用して Chromium を Build してみる という記事の中で、mold と呼ばれる「高速なリンカ」について紹介しました。\nhttps://github.com/rui314/mold\nmold は、自分の知る限りでは現時点では特に Binary の配信などは行っていないようです。利用したい場合には repository を git clone して、自分で Build して利用する必要があります。\nmold の Build 手順についてメモ程度に記録を残しておこうと思います。\n2021年3月20日追記: ちょうど4日ほど前に mold の README に How to build というセクションが追加されたようです。最新のソースコードで Build する場合は、そちらを参照してみてください。cf. https://github.com/rui314/mold#how-to-build\nステップ1. mold のソースコードを取得する まず、mold の git repository を clone します。\nminami@chromium-dev-20210227:~$ git clone https://github.com/rui314/mold.git minami@chromium-dev-20210227:~$ ls mold minami@chromium-dev-20210227:~$ cd mold/ mold は自身が依存する mimalloc と oneTBB を git submodule として利用しているので、git submodule update --init を実行してこれらのソースコードを取得します。","title":"mold の Build 手順メモ"},{"content":"はじめに 現在、広く使われているリンカの中でもっとも高速なものとして有名なのは LLVM project の LLD でしょう。LLD のパフォーマンスについては、公式 document に以下のような benchmark が掲載されていて、GNU ld, GNU gold などと比較して圧倒的に早いという結果が示されています。\nProgram\t| Output size | GNU ld\t| GNU gold w/o threads | GNU gold w/threads | lld w/o threads | lld w/threads ffmpeg dbg\t| 92 MiB | 1.72s | 1.16s\t| 1.01s | 0.60s\t| 0.35s mysqld dbg\t| 154 MiB\t| 8.50s | 2.96s\t| 2.68s | 1.06s\t| 0.68s clang dbg\t| 1.67 GiB\t| 104.03s\t| 34.18s\t| 23.49s\t| 14.82s\t| 5.28s chromium dbg\t| 1.14 GiB\t| 209.05s [1]\t| 64.70s\t| 60.82s\t| 27.60s\t| 16.70s cf. https://lld.llvm.org/#performance\nChromium を Checking out and building Chromium on Linux の手順にしたがって Build する場合、デフォルトで LLD が利用されるようになっています。そのため、何もせずとも「高速なリンク」という恩恵を受けることができるようになっています。\n一方、LLD の author である Rui Ueyama さんが最近活発に開発しているのが mold と呼ばれるリンカです。\nhttps://github.com/rui314/mold\nこちらは個人 project として開発を進めているようなのですが、既にかなりの完成度のようで、「LLD 以上に高速なリンク」を実現しているようです。\nmold、完全に目処がついたと言ってよさそう。Chromeをリンクするのに2.5秒、直前に同一コマンドを実行してファイルをプリロードしておくと、ほとんどの入力ファイルが同じ場合に0.8秒という結果になった（16スレッドでテスト）。ほかのリンカだと最低10秒かかるから、めちゃくちゃ速いといっていい。\n\u0026mdash; Rui Ueyama (@rui314) December 22, 2020 今日は、この「最も高速なリンカである mold」を利用した Chromium の Build を試してみたいと思います。\nステップ1. Linux マシンを用意する この部分は 前回 と同様です。\nGCP の Compute Engine で以下の VM Instance を立ててそこで作業を行うことにします。\n8core, 32GiB memory (E2, e2-standard-8) 200GB SSD image: Ubuntu 20.04 LTS zone: asia-northeast1-b 以下のコマンドで ssh して、そこで作業を行います。\n$ gcloud beta compute ssh --zone \u0026#34;asia-northeast1-b\u0026#34; \u0026lt;instance 名\u0026gt; ステップ2. mold を Build する mold は、自分の知る限りでは現時点では特に Binary の配信などは行っていないようです。利用したい場合には https://github.com/rui314/mold を git clone して、自分で Build して利用する必要があります。\nこの部分の手順は別途またブログにまとめたいと思います。 追記: この部分の手順は mold の Build 手順メモ に記載しました。そちらを参照してみてください。\nmold Binary が生成されて、以下のように利用できるようになっていれば OK です。\nminami@chromium-dev-20210227:~$ git clone https://github.com/rui314/mold.git # ここで、mold を Build minami@chromium-dev-20210227:~$ ls -l /home/minami/mold/mold -rwxrwxr-x 1 minami minami 11142376 Feb 27 01:43 /home/minami/mold/mold ステップ3. Chromium の Build 環境を整える。 Chromium を Build して動かすまでの待ち時間を「7 時間」から「30 分」まで高速化してみる を参照して、Chromium の Build 環境を整えます。30分 もかからずに、chrome Binary を Build できる環境が整うはずです。\nステップ4. chrome Binary の Build に利用されているリンカを確認しておく ここでは、事前に「$ autoninja -C out/Default chrome で Build をしたときに chrome Binary のリンクに利用されていたリンカは何者なのか」をチェックしてみます。\n$ autoninja -C out/Default chrome を実行して、[4/4] LINK ./chrome のタイミングで起動している process を $ ps fax で見てみます。そうすると、以下のように /home/minami/chromium/src/out/Default/../../third_party/llvm-build/Release+Asserts/bin/ld.lld が利用されていることが分かります。\nminami@chromium-dev-20210227:~$ ps fax PID TTY STAT TIME COMMAND . . . 1343 pts/0 S+ 0:00 | \\_ bash /home/minami/depot_tools/autoninja -C out/Default chrome 1455 pts/0 S+ 0:07 | \\_ /home/minami/depot_tools/ninja-linux64 -C out/Default chrome -j 10 1484 pts/0 S 0:00 | \\_ /bin/sh -c python \u0026#34;../../build/toolchain/gcc_link_wrapper.py\u0026#34; --output=\u0026#34;./chrome\u0026#34; -- ../../third_party/llvm-build/Release+Asserts/bin/clang++ -fuse-ld=lld -Wl,--fatal-warnings -Wl,--build-id -fPIC -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,defs -Wl,--as-needed -Wl,--color-diagnostics -Wl,--no-call-graph-profile-sor 1485 pts/0 S 0:00 | \\_ python ../../build/toolchain/gcc_link_wrapper.py --output=./chrome -- ../../third_party/llvm-build/Release+Asserts/bin/clang++ -fuse-ld=lld -Wl,--fatal-warnings -Wl,--build-id -fPIC -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,defs -Wl,--as-needed -Wl,--color-diagnostics -Wl,--no-call-graph-profile-sort -m64 -Wer 1486 pts/0 S 0:00 | \\_ ../../third_party/llvm-build/Release+Asserts/bin/clang++ -fuse-ld=lld -Wl,--fatal-warnings -Wl,--build-id -fPIC -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,defs -Wl,--as-needed -Wl,--color-diagnostics -Wl,--no-call-graph-profile-sort -m64 -Werror -Wl,--gdb-index -rdynamic -nostdlib++ --sysroot=../../build/li 1487 pts/0 D 0:03 | \\_ /home/minami/chromium/src/out/Default/../../third_party/llvm-build/Release+Asserts/bin/ld.lld @/tmp/response-96ee6b.txt /home/minami/chromium/src/out/Default/../../third_party/llvm-build/Release+Asserts/bin direcotory は以下のように clang や lld が入っていて、「LLVM project の toolchain が格納された directory」のようです。\nminami@chromium-dev-20210227:~$ cd /home/minami/chromium/src/out/Default/../../third_party/llvm-build/Release+Asserts/bin minami@chromium-dev-20210227:~/chromium/src/third_party/llvm-build/Release+Asserts/bin$ ls clang clang++ clang-cl ld64.lld ld64.lld.darwinnew ld.lld lld lld-link llvm-ar llvm-objcopy llvm-pdbutil llvm-symbolizer llvm-undname ld.lld は lld への symlink が貼られています。これが LLVM project の高速なリンカである LLD です。\nminami@chromium-dev-20210227:~/chromium/src/third_party/llvm-build/Release+Asserts/bin$ ls -la ld.lld lrwxrwxrwx 1 minami minami 3 Dec 12 12:50 ld.lld -\u0026gt; lld minami@chromium-dev-20210227:~/chromium/src/third_party/llvm-build/Release+Asserts/bin$ ./ld.lld --help OVERVIEW: lld USAGE: ./ld.lld [options] file... . . . ./ld.lld: supported targets: elf LLD が利用されていることは、生成された chrome Binary からも確かめることができます。LLVM project の document である Using LLD - LLVM には以下のように「readelf コマンドで .comment section を読み取ると Linker: LLD という記述があるはず」と記載されています。\nLLD leaves its name and version number to a .comment section in an output. If you are in doubt whether you are successfully using LLD or not, run readelf --string-dump .comment \u0026lt;output-file\u0026gt; and examine the output. If the string “Linker: LLD” is included in the output, you are using LLD.\n実際に「Build した chrome Binary」に対して readelf を実行してみると、確かに Linker: LLD 12.0.0 という記述を見つけることができます。\nminami@chromium-dev-20210227:~/chromium/src$ readelf --string-dump .comment out/Default/chrome String dump of section \u0026#39;.comment\u0026#39;: [ 0] GCC: (Debian 7.5.0-3) 7.5.0 [ 1c] clang version 12.0.0 (https://github.com/llvm/llvm-project/ 6ee22ca6ceb71661e8dbc296b471ace0614c07e5) [ 82] Linker: LLD 12.0.0 (https://github.com/llvm/llvm-project/ 6ee22ca6ceb71661e8dbc296b471ace0614c07e5) ここまで、利用されているリンカが何なのかを確認しました。それ以外に、chrome Binary の Build の際に実行される script もチェックしておきます。これは、autoninja コマンドに -v オプションをつけることで出力することができます。この情報は後々利用します。\nminami@chromium-dev-20210227:~/chromium/src$ time autoninja -v -C out/Default chrome ninja: Entering directory `out/Default\u0026#39; [1/4] ../../third_party/llvm-build/Release+Asserts/bin/clang++ -MMD -MF obj/chrome/common/channel_info/channel_info.o.d -DUSE_UDEV -DUSE_AURA=1 -DUSE_GLIB=1 -DUSE_NSS_CERTS=1 -DUSE_OZONE=1 -DUSE_X11=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_GNU_SOURCE -DCR_CLANG_REVISION=\\\u0026#34;llvmorg-12-init-12923-g6ee22ca6-1\\\u0026#34; -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DCOMPONENT_BUILD -D_LIBCPP_ABI_UNSTABLE -D_LIBCPP_ABI_VERSION=Cr -D_LIBCPP_ENABLE_NODISCARD -D_LIBCPP_DEBUG=0 -DCR_LIBCXX_REVISION=375504 -DCR_SYSROOT_HASH=22f2db7711f7426a364617bb6d78686cce09a8f9 -D_DEBUG -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DGLIB_VERSION_MAX_ALLOWED=GLIB_VERSION_2_40 -DGLIB_VERSION_MIN_REQUIRED=GLIB_VERSION_2_40 -DWEBP_EXTERN=extern -DABSL_CONSUME_DLL -DBORINGSSL_SHARED_LIBRARY -I../.. -Igen -I../../third_party/perfetto/include -Igen/third_party/perfetto/build_config -Igen/third_party/perfetto -I../../third_party/libwebp/src -I../../third_party/abseil-cpp -I../../third_party/boringssl/src/include -I../../third_party/protobuf/src -Igen/protoc_out -fno-delete-null-pointer-checks -fno-strict-aliasing --param=ssp-buffer-size=4 -fstack-protector -funwind-tables -fPIC -pthread -fcolor-diagnostics -fmerge-all-constants -fcrash-diagnostics-dir=../../tools/clang/crashreports -mllvm -instcombine-lower-dbg-declare=0 -fcomplete-member-pointers -m64 -march=x86-64 -msse3 -Wno-builtin-macro-redefined -D__DATE__= -D__TIME__= -D__TIMESTAMP__= -Xclang -fdebug-compilation-dir -Xclang . -no-canonical-prefixes -Wall -Werror -Wextra -Wimplicit-fallthrough -Wunreachable-code -Wthread-safety -Wextra-semi -Wno-missing-field-initializers -Wno-unused-parameter -Wno-c++11-narrowing -Wno-unneeded-internal-declaration -Wno-undefined-var-template -Wno-psabi -Wno-ignored-pragma-optimize -Wno-implicit-int-float-conversion -Wno-final-dtor-non-final-class -Wno-builtin-assume-aligned-alignment -Wno-deprecated-copy -Wno-non-c-typedef-for-linkage -Wmax-tokens -O0 -fno-omit-frame-pointer -g2 -Xclang -debug-info-kind=constructor -gsplit-dwarf -ggnu-pubnames -ftrivial-auto-var-init=pattern -fvisibility=hidden -Xclang -add-plugin -Xclang find-bad-constructs -Xclang -plugin-arg-find-bad-constructs -Xclang check-ipc -Wheader-hygiene -Wstring-conversion -Wtautological-overlap-compare -isystem../../build/linux/debian_sid_amd64-sysroot/usr/include/glib-2.0 -isystem../../build/linux/debian_sid_amd64-sysroot/usr/lib/x86_64-linux-gnu/glib-2.0/include -DPROTOBUF_ALLOW_DEPRECATED=1 -Wno-undefined-bool-conversion -Wno-tautological-undefined-compare -std=c++14 -fno-trigraphs -Wno-trigraphs -fno-exceptions -fno-rtti -nostdinc++ -isystem../../buildtools/third_party/libc++/trunk/include -isystem../../buildtools/third_party/libc++abi/trunk/include --sysroot=../../build/linux/debian_sid_amd64-sysroot -fvisibility-inlines-hidden -c ../../chrome/common/channel_info.cc -o obj/chrome/common/channel_info/channel_info.o [2/4] touch obj/chrome/common/channel_info.stamp [3/4] python \u0026#34;../../build/toolchain/gcc_solink_wrapper.py\u0026#34; --readelf=\u0026#34;readelf\u0026#34; --nm=\u0026#34;nm\u0026#34; --sofile=\u0026#34;./libvr_common.so\u0026#34; --tocfile=\u0026#34;./libvr_common.so.TOC\u0026#34; --output=\u0026#34;./libvr_common.so\u0026#34; -- ../../third_party/llvm-build/Release+Asserts/bin/clang++ -shared -Wl,-soname=\u0026#34;libvr_common.so\u0026#34; -fuse-ld=lld -Wl,--fatal-warnings -Wl,--build-id -fPIC -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,defs -Wl,--as-needed -Wl,--color-diagnostics -Wl,--no-call-graph-profile-sort -m64 -Werror -Wl,--gdb-index -rdynamic -nostdlib++ --sysroot=../../build/linux/debian_sid_amd64-sysroot -L../../build/linux/debian_sid_amd64-sysroot/usr/local/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/usr/lib/x86_64-linux-gnu -Wl,-rpath=\\$ORIGIN -o \u0026#34;./libvr_common.so\u0026#34; @\u0026#34;./libvr_common.so.rsp\u0026#34; [4/4] python \u0026#34;../../build/toolchain/gcc_link_wrapper.py\u0026#34; --output=\u0026#34;./chrome\u0026#34; -- ../../third_party/llvm-build/Release+Asserts/bin/clang++ -fuse-ld=lld -Wl,--fatal-warnings -Wl,--build-id -fPIC -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,defs -Wl,--as-needed -Wl,--color-diagnostics -Wl,--no-call-graph-profile-sort -m64 -Werror -Wl,--gdb-index -rdynamic -nostdlib++ --sysroot=../../build/linux/debian_sid_amd64-sysroot -L../../build/linux/debian_sid_amd64-sysroot/usr/local/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/usr/lib/x86_64-linux-gnu -pie -Wl,--disable-new-dtags -Wl,-rpath=\\$ORIGIN -o \u0026#34;./chrome\u0026#34; -Wl,--start-group @\u0026#34;./chrome.rsp\u0026#34; ./libbase.so ./libabsl.so ./libboringssl.so ./libperfetto.so ./libbindings.so ./libbindings_base.so ./libmojo_public_system_cpp.so ./libmojo_public_system.so ./libmojo_cpp_platform.so ./libmessage_support.so ./libmojo_mojom_bindings.so ./libmojo_mojom_bindings_shared.so ./liburl_mojom_traits.so ./libmojo_base_mojom_shared.so ./libmojo_base_shared_typemap_traits.so ./libmojo_base_lib.so ./libbase_i18n.so ./libicui18n.so ./libicuuc.so ./liburl.so ./libui_base.so ./libui_base_features.so ./libui_data_pack.so ./libskia.so ./libgfx.so ./libcolor_space.so ./libcolor_utils.so ./libgeometry.so ./libgeometry_skia.so ./libgfx_switches.so ./libanimation.so ./libcodec.so ./librange.so ./libcc_paint.so ./libcc_base.so ./libcc_debug.so ./libfile_info.so ./libevents_base.so ./libplatform.so ./libkeycodes_x11.so ./libui_base_x.so ./libcontent_public_common_mojo_bindings_shared.so ./libmojom_platform_shared.so ./libandroid_mojo_bindings_shared.so ./libauthenticator_test_mojo_bindings_shared.so ./libcolor_scheme_mojo_bindings_shared.so ./libmojom_mhtml_load_result_shared.so ./libscript_type_mojom_shared.so ./libweb_feature_mojo_bindings_mojom_shared.so ./libservice_manager_mojom_shared.so ./libservice_manager_mojom_constants_shared.so ./libdom_storage_mojom_shared.so ./libframe_mojom_shared.so ./libblink_gpu_mojom_shared.so ./libservice_worker_storage_mojom_shared.so ./libtokens_mojom_shared.so ./libusb_shared.so ./libmojo_base_mojom.so ./libmojo_base_typemap_traits.so ./libcontent_settings_features.so ./libipc.so ./libipc_mojom.so ./libipc_mojom_shared.so ./libprotobuf_lite.so ./libtracing_cpp.so ./libstartup_tracing.so ./libtracing_mojom.so ./libtracing_mojom_shared.so ./libnet.so ./libcrcrypto.so ./libskia_shared_typemap_traits.so ./libcontent.so ./libgpu.so ./libmailbox.so ./libcrash_key_lib.so ./libchrome_zlib.so ./libvulkan_info.so ./libgfx_ipc.so ./libgfx_ipc_geometry.so ./libvulkan_ycbcr_info.so ./liburl_ipc.so ./libviz_common.so ./libviz_resource_format_utils.so ./libviz_vulkan_context_provider.so ./libdisplay.so ./libdisplay_types.so ./libgl_wrapper.so ./libmedia.so ./libshared_memory_support.so ./libleveldb_proto.so ./libkeyed_service_core.so ./libleveldatabase.so ./libgfx_ipc_color.so ./libgfx_ipc_buffer_types.so ./libgfx_ipc_skia.so ./libgfx_native_types_shared_mojom_traits.so ./libgfx_shared_mojom_traits.so ./libgpu_shared_mojom_traits.so ./liblearning_common.so ./libmedia_learning_shared_typemap_traits.so ./libmedia_session_base_cpp.so ./libcookies_mojom_support.so ./libnetwork_cpp_base.so ./libcrash_keys.so ./libcross_origin_embedder_policy.so ./libip_address_mojom_support.so ./libschemeful_site_mojom_support.so ./libwebrtc_component.so ./libservice_manager_mojom.so ./libservice_manager_mojom_constants.so ./libservice_manager_cpp_types.so ./libservice_manager_mojom_traits.so ./libservice_manager_cpp.so ./libmetrics_cpp.so ./libui_base_clipboard_types.so ./libevents.so ./libui_base_cursor_base.so ./libdisplay_shared_mojom_traits.so ./libcc.so ./libvideo_capture_mojom_support.so ./libcapture_base.so ./liblatency_shared_mojom_traits.so ./libprediction.so ./libblink_common.so ./libprivacy_budget.so ./libnetwork_cpp.so ./libweb_feature_mojo_bindings_mojom.so ./libmojom_modules_shared.so ./libmojom_core_shared.so ./libfido.so ./libbluetooth.so ./libscript_type_mojom.so ./libcc_ipc.so ./libcc_shared_mojom_traits.so ./libdom_storage_mojom.so ./libframe_mojom.so ./libblink_gpu_mojom.so ./libservice_worker_storage_mojom.so ./libtokens_traits.so ./libime_shared_mojom_traits.so ./libui_base_ime_types.so ./libui_events_ipc.so ./libweb_bluetooth_mojo_bindings_shared.so ./libax_base.so ./libui_accessibility_ax_mojom.so ./libui_accessibility_ax_mojom_shared.so ./libui_base_ime.so ./libcontent_common_mojo_bindings_shared.so ./libaccessibility.so ./libgfx_x11.so ./libxprotos.so ./libaura.so ./libcompositor.so ./libblink_features.so ./libsurface.so ./libpolicy.so ./libnetwork_service.so ./libmemory_instrumentation.so ./libresource_coordinator_public_mojom.so ./libresource_coordinator_public_mojom_shared.so ./libstorage_common.so ./libpublic.so ./libinterfaces_shared.so ./libstorage_service_filesystem_mojom_shared.so ./libstorage_service_filesystem_mojom.so ./libstorage_service_typemap_traits.so ./libmedia_session_cpp.so ./libstorage_browser.so ./libvr_public_cpp.so ./libdevice_vr_isolated_xr_service_mojo_bindings.so ./libdevice_vr_isolated_xr_service_mojo_bindings_shared.so ./libdevice_vr_test_mojo_bindings_shared.so ./libdevice_vr_service_mojo_bindings_shared.so ./libgamepad_mojom_shared.so ./libdevice_vr_test_mojo_bindings.so ./libdevice_vr_service_mojo_bindings.so ./libgamepad_mojom.so ./libgamepad_shared_typemap_traits.so ./libshared_with_blink.so ./libdevice_vr_public_typemaps.so ./libchrome_features.so ./libprefs.so ./libvariations_features.so ./liburl_matcher.so ./libcapture_lib.so ./libmedia_webrtc.so ./libwtf.so ./libcommon.so ./libnetwork_session_configurator.so ./libsql.so ./libchromium_sqlite3.so ./libwebdata_common.so ./libos_crypt.so ./libomnibox_http_headers.so ./libcloud_policy_proto_generated_compile.so ./libpolicy_component.so ./libpolicy_proto.so ./libgcm.so ./libnative_theme.so ./libservice_provider.so ./libui_message_center_cpp.so ./libppapi_shared.so ./libmojo_core_embedder.so ./libprinting.so ./libsandbox_services.so ./libsuid_sandbox_client.so ./libseccomp_bpf.so ./libsecurity_state_features.so ./libui_base_clipboard.so ./libui_base_data_transfer_policy.so ./libkeyed_service_content.so ./libuser_prefs.so ./libextras.so ./libsessions.so ./libcaptive_portal_core.so ./libdevice_features.so ./libweb_modal.so ./libdevice_event_log.so ./libshell_dialogs.so ./libui_base_idle.so ./libdbus.so ./libonc.so ./libhost.so ./libukm_recorder.so ./libcrdtp.so ./libuser_manager.so ./libperformance_manager_public_mojom.so ./libperformance_manager_public_mojom_shared.so ./libviews.so ./libui_base_ime_init.so ./libui_base_cursor_theme_manager.so ./libui_base_cursor.so ./libx11_window.so ./libui_touch_selection.so ./libproxy_config.so ./libtab_groups.so ./libmanager.so ./libmessage_center.so ./libfontconfig.so ./libx11_events_platform.so ./libdevices.so ./libevents_devices_x11.so ./libevents_x.so ./libffmpeg.so ./libwebview.so ./libdomain_reliability.so ./liblookalikes_features.so ./libui_devtools.so ./libdata_exchange.so ./libgesture_detection.so ./libsnapshot.so ./libweb_dialogs.so ./libcolor.so ./libmixers.so ./libdiscardable_memory_service.so ./libAPP_UPDATE.so ./libozone.so ./libozone_base.so ./libdisplay_util.so ./libvulkan_wrapper.so ./libplatform_window.so ./libui_base_ime_linux.so ./libfreetype_harfbuzz.so ./libmenu.so ./libproperties.so ./libthread_linux.so ./libgtk.so ./libgtk_ui_delegate.so ./libbrowser_ui_views.so ./libwm.so ./libmedia_message_center.so ./libtab_count_metrics.so ./libui_gtk_x.so ./libwm_public.so ./libppapi_host.so ./libppapi_proxy.so ./libcertificate_matching.so ./libdevice_base.so ./libswitches.so ./libcapture_switches.so ./libmidi.so ./libmedia_mojo_services.so ./libmedia_gpu.so ./libgles2_utils.so ./libgles2.so ./libgpu_ipc_service.so ./libgl_init.so ./libcert_net_url_loader.so ./liberror_reporting.so ./libevents_ozone.so ./libschema_org_common.so ./libmirroring_service.so ./libvr_common.so ./libvr_base.so ./libdevice_vr.so ./libblink_controller.so ./libblink_core.so ./libblink_mojom_broadcastchannel_bindings_shared.so ./libwtf_support.so ./libweb_feature_mojo_bindings_mojom_blink.so ./libmojo_base_mojom_blink.so ./libservice_manager_mojom_blink.so ./libservice_manager_mojom_constants_blink.so ./libblink_platform.so ./libcc_animation.so ./libresource_coordinator_public_mojom_blink.so ./libv8.so ./libblink_embedded_frame_sink_mojo_bindings_shared.so ./libperformance_manager_public_mojom_blink.so ./libui_accessibility_ax_mojom_blink.so ./libgin.so ./libblink_modules.so ./libgamepad_mojom_blink.so ./liburlpattern.so ./libdevice_vr_service_mojo_bindings_blink.so ./libdevice_vr_test_mojo_bindings_blink.so ./libdiscardable_memory_client.so ./libcbor.so ./libpdfium.so ./libheadless_non_renderer.so ./libc++.so -Wl,--end-group -ldl -lpthread -lrt -lgmodule-2.0 -lgobject-2.0 -lgthread-2.0 -lglib-2.0 -lnss3 -lnssutil3 -lsmime3 -lplds4 -lplc4 -lnspr4 -latk-1.0 -latk-bridge-2.0 -lcups -ldbus-1 -lgio-2.0 -lexpat real 0m15.202s user 0m19.157s sys 0m4.398s ステップ5. mold を利用して chrome Binary をリンクしてみる さて、ここまでで「chrome のリンクに LLD が利用されていること」、「コマンドとしては /home/minami/chromium/src/out/Default/../../third_party/llvm-build/Release+Asserts/bin/ld.lld が利用されていること」が確認できました。\n次は、LLD の代わりに mold を利用してみたいと思います。ここでは、「ld.lld の symlink の向き先を lld から mold に切り替えて、Build する」というアプローチをとってみます。 以下のように ld.lld を消して、symlink の向き先を /home/minami/mold/mold に変えてみます。\nminami@chromium-dev-20210227:~$ cd /home/minami/chromium/src/out/Default/../../third_party/llvm-build/Release+Asserts/bin minami@chromium-dev-20210227:~/chromium/src/third_party/llvm-build/Release+Asserts/bin$ rm ld.lld minami@chromium-dev-20210227:~/chromium/src/third_party/llvm-build/Release+Asserts/bin$ ln -s /home/minami/mold/mold ld.lld minami@chromium-dev-20210227:~/chromium/src/third_party/llvm-build/Release+Asserts/bin$ ls -la ld.lld lrwxrwxrwx 1 minami minami 22 Feb 28 03:06 ld.lld -\u0026gt; /home/minami/mold/mold これで、mold が利用されるようになるはずです。この状態で再度 chrome の Build をしてみます。\nただ、この状態で $ autoninja -C out/Default chrome を実行すると、以下のように [3/4] SOLINK ./libvr_common.so のステップでリンクに失敗してしまします。\nminami@chromium-dev-20210227:~/chromium/src$ time autoninja -C out/Default chrome ninja: Entering directory `out/Default\u0026#39; [3/4] SOLINK ./libvr_common.so FAILED: libvr_common.so libvr_common.so.TOC python \u0026#34;../../build/toolchain/gcc_solink_wrapper.py\u0026#34; --readelf=\u0026#34;readelf\u0026#34; --nm=\u0026#34;nm\u0026#34; --sofile=\u0026#34;./libvr_common.so\u0026#34; --tocfile=\u0026#34;./libvr_common.so.TOC\u0026#34; --output=\u0026#34;./libvr_common.so\u0026#34; -- ../../third_party/llvm-build/Release+Asserts/bin/clang++ -shared -Wl,-soname=\u0026#34;libvr_common.so\u0026#34; -fuse-ld=lld -Wl,--fatal-warnings -Wl,--build-id -fPIC -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,defs -Wl,--as-needed -Wl,--color-diagnostics -Wl,--no-call-graph-profile-sort -m64 -Werror -Wl,--gdb-index -rdynamic -nostdlib++ --sysroot=../../build/linux/debian_sid_amd64-sysroot -L../../build/linux/debian_sid_amd64-sysroot/usr/local/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/usr/lib/x86_64-linux-gnu -Wl,-rpath=\\$ORIGIN -o \u0026#34;./libvr_common.so\u0026#34; @\u0026#34;./libvr_common.so.rsp\u0026#34; mold: unknown command line option: -soname=libvr_common.so clang: error: linker command failed with exit code 1 (use -v to see invocation) ninja: build stopped: subcommand failed. real 0m5.480s user 0m4.577s sys 0m0.746s mold が -soname オプションをサポートしてないために、エラーが出ているようです。\n上記の python \u0026quot;../../build/toolchain/gcc_solink_wrapper.py\u0026quot; ... という部分が [3/4] SOLINK ./libvr_common.so のステップとして実際に実行されているコマンドです。ここから、「mold がサポートしていないオプション」を消して、同じコマンドを手動で実行してみます。具体的には、-soname , --color-diagnostics, --no-call-graph-profile-sort, --gdb-index オプションの指定を消して、以下のように実行します。こうすると、ちゃんと mold によるリンクに成功して、 libvr_common.so という Shared Object File が生成されます。\nminami@chromium-dev-20210227:~/chromium/src/out/Default$ time python \u0026#34;../../build/toolchain/gcc_solink_wrapper.py\u0026#34; --readelf=\u0026#34;readelf\u0026#34; --nm=\u0026#34;nm\u0026#34; --sofile=\u0026#34;./libvr_common.so\u0026#34; --tocfile=\u0026#34;./libvr_common.so.TOC\u0026#34; --output=\u0026#34;./libvr_common.so\u0026#34; -- ../../third_party/llvm-build/Release+Asserts/bin/clang++ -shared -fuse-ld=lld -Wl,--fatal-warnings -Wl,--build-id -fPIC -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,defs -Wl,--as-needed -m64 -Werror -rdynamic -nostdlib++ --sysroot=../../build/linux/debian_sid_amd64-sysroot -L../../build/linux/debian_sid_amd64-sysroot/usr/local/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/usr/lib/x86_64-linux-gnu -Wl,-rpath=\\$ORIGIN -o \u0026#34;./libvr_common.so\u0026#34; @\u0026#34;./libvr_common.so.rsp\u0026#34; real 0m0.563s user 0m0.033s sys 0m0.029s minami@chromium-dev-20210227:~/chromium/src/out/Default$ ls -la libvr_common.so -rwxrwxr-x 1 minami minami 192508187 Feb 28 03:14 libvr_common.so 上記の python script の実行が、[3/4] SOLINK ./libvr_common.so に相当する処理でした。さらに、 [4/4] LINK ./chrome に相当する処理も、直接 python script の実行を行う事にします。これは、ステップ4の最後に $ autoninja -v -C out/Default chrome コマンドで出力したコマンドの、[4/4] python \u0026quot;../../build/toolchain/gcc_link_wrapper.py\u0026quot; --output=\u0026quot;./chrome\u0026quot; -- ...(長いので省略) が該当します。\nただ、この状態で [4/4] LINK ./chromeに相当する python script を実行しても、以下のように chrome.rsp という File が無いことで失敗してしまいます。\nminami@chromium-dev-20210227:~/chromium/src/out/Default$ python \u0026#34;../../build/toolchain/gcc_link_wrapper.py\u0026#34; --output=\u0026#34;./chrome\u0026#34; -- ...(長いので省略) clang: error: no such file or directory: \u0026#39;@./chrome.rsp\u0026#39; そこで、一度 ld.lld の symlink 先を lld に戻してから、$ autoninja -C out/Default chrome の実行中に [4/4] LINK ./chrome のタイミングで Ctrl-C で python script を強制 exit してみます。 こうすることで、「libvr_common.so とchrome.rsp が存在する状態（ちょうど [4/4] LINK ./chrome の開始前の状態」を再現することが出来ます。\nminami@chromium-dev-20210227:~/chromium/src$ ls out/Default/libvr_common.so out/Default/libvr_common.so minami@chromium-dev-20210227:~/chromium/src$ ls out/Default/chrome.rsp out/Default/chrome.rsp この状態で、再度 ld.lld の symlink 先を mold にしてから、[4/4] LINK ./chrome に相当する python script を手動で実行します。mold でサポートされてない --color-diagnostics, --no-call-graph-profile-sort, --gdb-indexオプションの指定は消しておきます。 こうすると、以下のようにちゃんとリンクに成功します。chrome Binary が生成されたことも確認できます。\nminami@chromium-dev-20210227:~/chromium/src/out/Default$ time python \u0026#34;../../build/toolchain/gcc_link_wrapper.py\u0026#34; --output=\u0026#34;./chrome\u0026#34; -- ../../third_party/llvm-build/Release+Asserts/bin/clang++ -fuse-ld=lld -Wl,--fatal-warnings -Wl,--build-id -fPIC -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,defs -Wl,--as-needed -m64 -Werror -rdynamic -nostdlib++ --sysroot=../../build/linux/debian_sid_amd64-sysroot -L../../build/linux/debian_sid_amd64-sysroot/usr/local/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/usr/lib/x86_64-linux-gnu -pie -Wl,--disable-new-dtags -Wl,-rpath=\\$ORIGIN -o \u0026#34;./chrome\u0026#34; -Wl,--start-group @\u0026#34;./chrome.rsp\u0026#34; ./libbase.so ./libabsl.so ./libboringssl.so ./libperfetto.so ./libbindings.so ./libbindings_base.so ./libmojo_public_system_cpp.so ./libmojo_public_system.so ./libmojo_cpp_platform.so ./libmessage_support.so ./libmojo_mojom_bindings.so ./libmojo_mojom_bindings_shared.so ./liburl_mojom_traits.so ./libmojo_base_mojom_shared.so ./libmojo_base_shared_typemap_traits.so ./libmojo_base_lib.so ./libbase_i18n.so ./libicui18n.so ./libicuuc.so ./liburl.so ./libui_base.so ./libui_base_features.so ./libui_data_pack.so ./libskia.so ./libgfx.so ./libcolor_space.so ./libcolor_utils.so ./libgeometry.so ./libgeometry_skia.so ./libgfx_switches.so ./libanimation.so ./libcodec.so ./librange.so ./libcc_paint.so ./libcc_base.so ./libcc_debug.so ./libfile_info.so ./libevents_base.so ./libplatform.so ./libkeycodes_x11.so ./libui_base_x.so ./libcontent_public_common_mojo_bindings_shared.so ./libmojom_platform_shared.so ./libandroid_mojo_bindings_shared.so ./libauthenticator_test_mojo_bindings_shared.so ./libcolor_scheme_mojo_bindings_shared.so ./libmojom_mhtml_load_result_shared.so ./libscript_type_mojom_shared.so ./libweb_feature_mojo_bindings_mojom_shared.so ./libservice_manager_mojom_shared.so ./libservice_manager_mojom_constants_shared.so ./libdom_storage_mojom_shared.so ./libframe_mojom_shared.so ./libblink_gpu_mojom_shared.so ./libservice_worker_storage_mojom_shared.so ./libtokens_mojom_shared.so ./libusb_shared.so ./libmojo_base_mojom.so ./libmojo_base_typemap_traits.so ./libcontent_settings_features.so ./libipc.so ./libipc_mojom.so ./libipc_mojom_shared.so ./libprotobuf_lite.so ./libtracing_cpp.so ./libstartup_tracing.so ./libtracing_mojom.so ./libtracing_mojom_shared.so ./libnet.so ./libcrcrypto.so ./libskia_shared_typemap_traits.so ./libcontent.so ./libgpu.so ./libmailbox.so ./libcrash_key_lib.so ./libchrome_zlib.so ./libvulkan_info.so ./libgfx_ipc.so ./libgfx_ipc_geometry.so ./libvulkan_ycbcr_info.so ./liburl_ipc.so ./libviz_common.so ./libviz_resource_format_utils.so ./libviz_vulkan_context_provider.so ./libdisplay.so ./libdisplay_types.so ./libgl_wrapper.so ./libmedia.so ./libshared_memory_support.so ./libleveldb_proto.so ./libkeyed_service_core.so ./libleveldatabase.so ./libgfx_ipc_color.so ./libgfx_ipc_buffer_types.so ./libgfx_ipc_skia.so ./libgfx_native_types_shared_mojom_traits.so ./libgfx_shared_mojom_traits.so ./libgpu_shared_mojom_traits.so ./liblearning_common.so ./libmedia_learning_shared_typemap_traits.so ./libmedia_session_base_cpp.so ./libcookies_mojom_support.so ./libnetwork_cpp_base.so ./libcrash_keys.so ./libcross_origin_embedder_policy.so ./libip_address_mojom_support.so ./libschemeful_site_mojom_support.so ./libwebrtc_component.so ./libservice_manager_mojom.so ./libservice_manager_mojom_constants.so ./libservice_manager_cpp_types.so ./libservice_manager_mojom_traits.so ./libservice_manager_cpp.so ./libmetrics_cpp.so ./libui_base_clipboard_types.so ./libevents.so ./libui_base_cursor_base.so ./libdisplay_shared_mojom_traits.so ./libcc.so ./libvideo_capture_mojom_support.so ./libcapture_base.so ./liblatency_shared_mojom_traits.so ./libprediction.so ./libblink_common.so ./libprivacy_budget.so ./libnetwork_cpp.so ./libweb_feature_mojo_bindings_mojom.so ./libmojom_modules_shared.so ./libmojom_core_shared.so ./libfido.so ./libbluetooth.so ./libscript_type_mojom.so ./libcc_ipc.so ./libcc_shared_mojom_traits.so ./libdom_storage_mojom.so ./libframe_mojom.so ./libblink_gpu_mojom.so ./libservice_worker_storage_mojom.so ./libtokens_traits.so ./libime_shared_mojom_traits.so ./libui_base_ime_types.so ./libui_events_ipc.so ./libweb_bluetooth_mojo_bindings_shared.so ./libax_base.so ./libui_accessibility_ax_mojom.so ./libui_accessibility_ax_mojom_shared.so ./libui_base_ime.so ./libcontent_common_mojo_bindings_shared.so ./libaccessibility.so ./libgfx_x11.so ./libxprotos.so ./libaura.so ./libcompositor.so ./libblink_features.so ./libsurface.so ./libpolicy.so ./libnetwork_service.so ./libmemory_instrumentation.so ./libresource_coordinator_public_mojom.so ./libresource_coordinator_public_mojom_shared.so ./libstorage_common.so ./libpublic.so ./libinterfaces_shared.so ./libstorage_service_filesystem_mojom_shared.so ./libstorage_service_filesystem_mojom.so ./libstorage_service_typemap_traits.so ./libmedia_session_cpp.so ./libstorage_browser.so ./libvr_public_cpp.so ./libdevice_vr_isolated_xr_service_mojo_bindings.so ./libdevice_vr_isolated_xr_service_mojo_bindings_shared.so ./libdevice_vr_test_mojo_bindings_shared.so ./libdevice_vr_service_mojo_bindings_shared.so ./libgamepad_mojom_shared.so ./libdevice_vr_test_mojo_bindings.so ./libdevice_vr_service_mojo_bindings.so ./libgamepad_mojom.so ./libgamepad_shared_typemap_traits.so ./libshared_with_blink.so ./libdevice_vr_public_typemaps.so ./libchrome_features.so ./libprefs.so ./libvariations_features.so ./liburl_matcher.so ./libcapture_lib.so ./libmedia_webrtc.so ./libwtf.so ./libcommon.so ./libnetwork_session_configurator.so ./libsql.so ./libchromium_sqlite3.so ./libwebdata_common.so ./libos_crypt.so ./libomnibox_http_headers.so ./libcloud_policy_proto_generated_compile.so ./libpolicy_component.so ./libpolicy_proto.so ./libgcm.so ./libnative_theme.so ./libservice_provider.so ./libui_message_center_cpp.so ./libppapi_shared.so ./libmojo_core_embedder.so ./libprinting.so ./libsandbox_services.so ./libsuid_sandbox_client.so ./libseccomp_bpf.so ./libsecurity_state_features.so ./libui_base_clipboard.so ./libui_base_data_transfer_policy.so ./libkeyed_service_content.so ./libuser_prefs.so ./libextras.so ./libsessions.so ./libcaptive_portal_core.so ./libdevice_features.so ./libweb_modal.so ./libdevice_event_log.so ./libshell_dialogs.so ./libui_base_idle.so ./libdbus.so ./libonc.so ./libhost.so ./libukm_recorder.so ./libcrdtp.so ./libuser_manager.so ./libperformance_manager_public_mojom.so ./libperformance_manager_public_mojom_shared.so ./libviews.so ./libui_base_ime_init.so ./libui_base_cursor_theme_manager.so ./libui_base_cursor.so ./libx11_window.so ./libui_touch_selection.so ./libproxy_config.so ./libtab_groups.so ./libmanager.so ./libmessage_center.so ./libfontconfig.so ./libx11_events_platform.so ./libdevices.so ./libevents_devices_x11.so ./libevents_x.so ./libffmpeg.so ./libwebview.so ./libdomain_reliability.so ./liblookalikes_features.so ./libui_devtools.so ./libdata_exchange.so ./libgesture_detection.so ./libsnapshot.so ./libweb_dialogs.so ./libcolor.so ./libmixers.so ./libdiscardable_memory_service.so ./libAPP_UPDATE.so ./libozone.so ./libozone_base.so ./libdisplay_util.so ./libvulkan_wrapper.so ./libplatform_window.so ./libui_base_ime_linux.so ./libfreetype_harfbuzz.so ./libmenu.so ./libproperties.so ./libthread_linux.so ./libgtk.so ./libgtk_ui_delegate.so ./libbrowser_ui_views.so ./libwm.so ./libmedia_message_center.so ./libtab_count_metrics.so ./libui_gtk_x.so ./libwm_public.so ./libppapi_host.so ./libppapi_proxy.so ./libcertificate_matching.so ./libdevice_base.so ./libswitches.so ./libcapture_switches.so ./libmidi.so ./libmedia_mojo_services.so ./libmedia_gpu.so ./libgles2_utils.so ./libgles2.so ./libgpu_ipc_service.so ./libgl_init.so ./libcert_net_url_loader.so ./liberror_reporting.so ./libevents_ozone.so ./libschema_org_common.so ./libmirroring_service.so ./libvr_common.so ./libvr_base.so ./libdevice_vr.so ./libblink_controller.so ./libblink_core.so ./libblink_mojom_broadcastchannel_bindings_shared.so ./libwtf_support.so ./libweb_feature_mojo_bindings_mojom_blink.so ./libmojo_base_mojom_blink.so ./libservice_manager_mojom_blink.so ./libservice_manager_mojom_constants_blink.so ./libblink_platform.so ./libcc_animation.so ./libresource_coordinator_public_mojom_blink.so ./libv8.so ./libblink_embedded_frame_sink_mojo_bindings_shared.so ./libperformance_manager_public_mojom_blink.so ./libui_accessibility_ax_mojom_blink.so ./libgin.so ./libblink_modules.so ./libgamepad_mojom_blink.so ./liburlpattern.so ./libdevice_vr_service_mojo_bindings_blink.so ./libdevice_vr_test_mojo_bindings_blink.so ./libdiscardable_memory_client.so ./libcbor.so ./libpdfium.so ./libheadless_non_renderer.so ./libc++.so -Wl,--end-group -ldl -lpthread -lrt -lgmodule-2.0 -lgobject-2.0 -lgthread-2.0 -lglib-2.0 -lnss3 -lnssutil3 -lsmime3 -lplds4 -lplc4 -lnspr4 -latk-1.0 -latk-bridge-2.0 -lcups -ldbus-1 -lgio-2.0 -lexpat real 0m3.312s user 0m0.064s sys 0m0.027s minami@chromium-dev-20210227:~/chromium/src/out/Default$ ls -la chrome -rwxrwxr-x 1 minami minami 1296141738 Feb 28 03:46 chrome この chrome の .comment section を見て、出自を確認してみましょう。LLD でリンクした時は Linker: LLD 12.0.0 という記述があったのに対して、この chrome Binary にはその記述がありません。逆説的に、「LLD 以外のリンカ（= mold）でリンクしたこと」が確認できたと言えそうです。\nminami@chromium-dev-20210227:~/chromium/src$ readelf --string-dump .comment out/Default/chrome String dump of section \u0026#39;.comment\u0026#39;: [ 1] GCC: (Debian 7.5.0-3) 7.5.0 [ 1d] clang version 12.0.0 (https://github.com/llvm/llvm-project/ 6ee22ca6ceb71661e8dbc296b471ace0614c07e5) 2021年3月27日追記: mold の README の How to use を見ると、今では .comment section に mold という文字列が commit hash つきで記載されるようになったようです（ただし、自分は動作未検証です）。\n生成した chrome Binary の挙動も確認してみましょう。以前のブログ記事 のように、「headless mode での実行」を試してみます。以下のようにちゃんと動作をすることが確認できました 🎉\nminami@chromium-dev-20210227:~/chromium/src$ ./out/Default/chrome --headless --disable-gpu --dump-dom https://example.com [0228/035317.179292:WARNING:headless_content_main_delegate.cc(530)] Cannot create Pref Service with no user data dir. [0228/035317.194952:INFO:content_main_runner_impl.cc(1027)] Chrome is running in full browser mode. \u0026lt;!DOCTYPE html\u0026gt; . . . ということで、無事「mold による chrome Binary のリンク」に成功しました。\n既存の Build の仕組みに組み込もうと思うと「mold ではサポートされてない option の扱い」などいくつか考える必要がありそうですが、試しにリンクをするだけであれば比較的簡単に実行ができることが分かりました。\nLLD と mold の速度比較 mold は「LLD よりも高速なリンカ」として開発されています。実際に、リンクにかかる時間がどれだけ変わったのか、比較してみましょう。ここでは、上記の「リンカに渡す option を減らした状態でのリンクの実行」を mold と LLD の両方で行って、time で計測した結果を載せています。\nLLD [3/4] SOLINK ./libvr_common.so 相当の処理 minami@chromium-dev-20210227:~/chromium/src/out/Default$ time python \u0026#34;../../build/toolchain/gcc_solink_wrapper.py\u0026#34; --readelf=\u0026#34;readelf\u0026#34; --nm=\u0026#34;nm\u0026#34; --sofile=\u0026#34;./libvr_common.so\u0026#34; --tocfile=\u0026#34;./libvr_common.so.TOC\u0026#34; --output=\u0026#34;./libvr_common.so\u0026#34; -- ../../third_party/llvm-build/Release+Asserts/bin/clang++ -shared -fuse-ld=lld -Wl,--fatal-warnings -Wl,--build-id -fPIC -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,defs -Wl,--as-needed -m64 -Werror -rdynamic -nostdlib++ --sysroot=../../build/linux/debian_sid_amd64-sysroot -L../../build/linux/debian_sid_amd64-sysroot/usr/local/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/usr/lib/x86_64-linux-gnu -Wl,-rpath=\\$ORIGIN -o \u0026#34;./libvr_common.so\u0026#34; @\u0026#34;./libvr_common.so.rsp\u0026#34; real 0m1.265s user 0m1.391s sys 0m0.837s [4/4] LINK ./chrome 相当の処理 minami@chromium-dev-20210227:~/chromium/src/out/Default$ time python \u0026#34;../../build/toolchain/gcc_link_wrapper.py\u0026#34; --output=\u0026#34;./chrome\u0026#34; -- ...(長いので省略) real 0m7.777s user 0m8.309s sys 0m3.664s mold [3/4] SOLINK ./libvr_common.so 相当の処理 minami@chromium-dev-20210227:~/chromium/src/out/Default$ time python \u0026#34;../../build/toolchain/gcc_solink_wrapper.py\u0026#34; --readelf=\u0026#34;readelf\u0026#34; --nm=\u0026#34;nm\u0026#34; --sofile=\u0026#34;./libvr_common.so\u0026#34; --tocfile=\u0026#34;./libvr_common.so.TOC\u0026#34; --output=\u0026#34;./libvr_common.so\u0026#34; -- ../../third_party/llvm-build/Release+Asserts/bin/clang++ -shared -fuse-ld=lld -Wl,--fatal-warnings -Wl,--build-id -fPIC -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,defs -Wl,--as-needed -m64 -Werror -rdynamic -nostdlib++ --sysroot=../../build/linux/debian_sid_amd64-sysroot -L../../build/linux/debian_sid_amd64-sysroot/usr/local/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/lib/x86_64-linux-gnu -L../../build/linux/debian_sid_amd64-sysroot/usr/lib/x86_64-linux-gnu -Wl,-rpath=\\$ORIGIN -o \u0026#34;./libvr_common.so\u0026#34; @\u0026#34;./libvr_common.so.rsp\u0026#34; real 0m0.563s user 0m0.033s sys 0m0.029s [4/4] LINK ./chrome 相当の処理 minami@chromium-dev-20210227:~/chromium/src/out/Default$ time python \u0026#34;../../build/toolchain/gcc_link_wrapper.py\u0026#34; --output=\u0026#34;./chrome\u0026#34; -- ...(長いので省略) real 0m3.312s user 0m0.064s sys 0m0.027s mold と LLD の比較 [3/4] SOLINK ./libvr_common.so 相当の処理については 1.265s -\u0026gt; 0.563s、[4/4] LINK ./chrome 相当の処理については 7.777s -\u0026gt; 3.312s とそれぞれ 2倍以上の高速化が達成できています 🎉\nなお、上記の比較を見ると十分に早いですが、それでも「はじめにで掲載した Rui Ueyama さんの Tweet」の 2.5秒という記述に比べると遅いです。これはおそらく、今回利用した GCP VM instance の「8core」という構成に起因するのではないかと考えています。mold は複数 CPU core をうまく活用する作りになっているようなので、CPU core 数が増えることで近いパフォーマンスが出るのだと思われます。\nまとめ Rui Ueyama さんが開発してる mold を利用して、Chromium の Build をしてみました。\n実験の結果、試しにリンクをするだけであれば比較的簡単に実行ができることが分かりました。また、CPU core 数が 8 とそれほど多くない構成であっても、 LLD に比べてリンクが2倍以上高速化する という改善が見られることが分かりました。\nRui Ueyama さんは自分が尊敬するエンジニアの1人なのですが、彼のエンジニアリングによって世界がより良くなっている事を体感できたように思います。\n","permalink":"https://south37.link/posts/20210228-chromium-with-mold/","summary":"はじめに 現在、広く使われているリンカの中でもっとも高速なものとして有名なのは LLVM project の LLD でしょう。LLD のパフォーマンスについては、公式 document に以下のような benchmark が掲載されていて、GNU ld, GNU gold などと比較して圧倒的に早いという結果が示されています。\nProgram\t| Output size | GNU ld\t| GNU gold w/o threads | GNU gold w/threads | lld w/o threads | lld w/threads ffmpeg dbg\t| 92 MiB | 1.72s | 1.16s\t| 1.01s | 0.60s\t| 0.35s mysqld dbg\t| 154 MiB\t| 8.50s | 2.96s\t| 2.68s | 1.06s\t| 0.68s clang dbg\t| 1.","title":"mold と呼ばれる高速なリンカを利用して Chromium を Build してみる"},{"content":"kuKubernetes の Install Tools というページでは、kubernetes を local で動かすための tool として kind が紹介されています。今日はこの kind について内部構造及び使い方を見てみます。\nkind とは kind は「Docker container の中で Kubernetes を動かすことが出来るツール」です。kind という命名は「Kubernetes in Docker」から来ていて、K-in-D という頭文字をとったものになっています。\nkind については KubeCon + CloudNativeCon で何度か紹介されているようです。例えば KubeCon + CloudNativeCon North America 2019 における以下の \u0026ldquo;Deep Dive: Kind\u0026rdquo; というトーク では、「kind とは何か？」について内部実装など含めて紹介されています。\n上記のトークについて簡単に summary を書くと、kind は以下のようなものとして紹介されています。\nDocker container として Node（をシミュレートする container）を動かし、その中で Kubernetes を動かすツール Node image の中に、Kubernetes を動かすために必要な全てを詰める kubelet kubeadm docker systemd core images (Kuberentes にとって重要な container image) etcd coredns pause kube-apiserver etc. multi-node 構成も可能 Kubernetes を source code から build して動かすことが可能 30s 程度で Kuberenetes cluster を作ることが出来る kind は「Kuberentes 自体の test」のために作られた Kuberenetes の CI は Kubernetes で動いており、全ての test は Pod の中で実行される。そのため、Kuberenetes 自体を test するには、「Kuberentes を container の中で動かす」必要があった。 上記のトークの中で出てくる以下のスライドが、kind の仕組みを端的に示していて分かりやすいと思います。\u0026ldquo;Node\u0026rdquo; Container の中で、kubelet や containerd, そして containered を通じて起動された「Kuberentes の動作を支える各種 container」が動いている事が分かります。\nここまでで、「kind の内部構造」を説明しました。次に、実際に kind を動かしてみましょう。\nkind を動かしてみる kind の利用方法は実はとても簡単で、README で以下のように言及されている通り「GO111MODULE=\u0026quot;on\u0026quot; go get sigs.k8s.io/kind@v0.9.0 \u0026amp;\u0026amp; kind create cluster を実行するだけ」となっています。\nIf you have go (1.11+) and docker installed GO111MODULE=\u0026quot;on\u0026quot; go get sigs.k8s.io/kind@v0.9.0 \u0026amp;\u0026amp; kind create cluster is all you need!\ncf. https://github.com/kubernetes-sigs/kind#please-see-our-documentation-for-more-in-depth-installation-etc\n実際に実行すると、以下のように Kubernetes cluster 作成が進みます。自分の環境では、Kuberentes cluster 作成にかかる時間は 1min 強でした。\n（なお、Ensuring node image という step で少し時間がかかりますが、これは後述する「kind が利用する kindest/node という docker image の pull に時間がかかっている」だけです。一度 image pull が完了すると次からは 30s 程度で高速に Kubernetes cluster が作成出来る様になります）。\n$ GO111MODULE=\u0026#34;on\u0026#34; go get sigs.k8s.io/kind@v0.9.0 \u0026amp;\u0026amp; kind create cluster go: downloading sigs.k8s.io/kind v0.9.0 go: downloading github.com/spf13/cobra v1.0.0 go: downloading k8s.io/apimachinery v0.18.8 go: downloading gopkg.in/yaml.v3 v3.0.0-20200615113413-eeeca48fe776 go: downloading github.com/mattn/go-isatty v0.0.12 go: downloading github.com/alessio/shellescape v1.2.2 go: downloading sigs.k8s.io/yaml v1.2.0 go: downloading github.com/pelletier/go-toml v1.8.0 go: downloading github.com/evanphx/json-patch v0.0.0-20200808040245-162e5629780b go: downloading golang.org/x/sys v0.0.0-20200814200057-3d37ad5750ed go: downloading github.com/evanphx/json-patch/v5 v5.1.0 GO111MODULE=\u0026#34;on\u0026#34; go get sigs.k8s.io/kind@v0.9.0 8.93s user 3.64s system 102% cpu 12.228 total Creating cluster \u0026#34;kind\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.19.1) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \u0026#34;kind-kind\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-kind Not sure what to do next? 😅 Check out https://kind.sigs.k8s.io/docs/user/quick-start/ kind create cluster 4.95s user 2.65s system 9% cpu 1:16.71 total これで既に「Kubernetes cluster が Docker container の中で動いている状態」となっています。簡単ですね！\nkind で動く Kubernetes cluster を利用してみる 次に、実際に kind で動く Kuberentes cluster を利用してみましょう。\n$ kind create cluster を実行した際の message に Set kubectl context to \u0026quot;kind-kind\u0026quot; と出ていたように、既に kubectl の context は「kind で作成した Kubenetes cluster（= kind-kind という名前の cluster）」に切り替わっています。つまり、この状態で $ kubectl を利用すれば、kind-kind cluster の API server に対して通信が行われるようになっています。\n実際、以下のように $ kubectl config current-context や $ kubectl cluster-info の結果を見てみると「local で動く kind-kind cluster に context が切り替わっている」事が確認出来ます。\n$ kubectl config current-context kind-kind $ kubectl cluster-info Kubernetes control plane is running at https://127.0.0.1:55999 KubeDNS is running at https://127.0.0.1:55999/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. $ kubectl で cluster 内の k8s object を見てみましょう。例えば namespace や pod, service を見てみると、以下のような内容になっています。\n$ kubectl get namespaces NAME STATUS AGE default Active 11m kube-node-lease Active 11m kube-public Active 11m kube-system Active 11m local-path-storage Active 11m $ kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-f9fd979d6-9cgbl 1/1 Running 0 11m kube-system coredns-f9fd979d6-wlnmw 1/1 Running 0 11m kube-system etcd-kind-control-plane 1/1 Running 0 11m kube-system kindnet-phgz9 1/1 Running 0 11m kube-system kube-apiserver-kind-control-plane 1/1 Running 0 11m kube-system kube-controller-manager-kind-control-plane 1/1 Running 0 11m kube-system kube-proxy-dxx9q 1/1 Running 0 11m kube-system kube-scheduler-kind-control-plane 1/1 Running 0 11m local-path-storage local-path-provisioner-78776bfc44-66wln 1/1 Running 0 11m $ kubectl get svc --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 18m kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 18m さらに、適当に deployment と service の追加もしてみましょう。自分が昔作った https://github.com/south37/dumper という「request header を print するだけの Docker container」を動かしてみます。\nまず、deployment と service の manifest file を用意します。\n# dumper-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: dumper name: dumper namespace: default spec: selector: matchLabels: app: dumper template: metadata: annotations: labels: app: dumper name: dumper spec: containers: - image: south37/dumper livenessProbe: httpGet: path: /ping port: 8080 name: dumper ports: - containerPort: 8080 name: http readinessProbe: httpGet: path: /ping port: 8080 # dumper-service.yaml apiVersion: v1 kind: Service metadata: labels: app: dumper name: dumper namespace: default spec: ports: - name: http port: 80 protocol: TCP targetPort: 8080 selector: app: dumper type: ClusterIP 次に、これらの manfest file を apply します。\n$ kubectl apply -f dumper-deployment.yaml deployment.apps/dumper created $ kubectl apply -f dumper-service.yaml service/dumper created apply したものがちゃんと作られている事を確認します。\n$ kubectl get all -n default NAME READY STATUS RESTARTS AGE pod/dumper-6465654fdc-qn729 1/1 Running 0 118s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dumper ClusterIP 10.110.60.42 \u0026lt;none\u0026gt; 80/TCP 114s service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 21m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/dumper 1/1 1 1 118s NAME DESIRED CURRENT READY AGE replicaset.apps/dumper-6465654fdc 1 1 1 118s まず、pod の log を見てみます。すると、healthcheck 用の GET /ping の reqeust が来ている事が確認できます。\n$ kubectl logs dumper-6465654fdc-qn729 . . . 2020/12/30 12:09:55 GET /ping HTTP/1.1 2020/12/30 12:09:55 Host: 10.244.0.5:8080 2020/12/30 12:09:55 User-Agent: kube-probe/1.19 2020/12/30 12:09:55 Accept-Encoding: gzip 2020/12/30 12:09:55 Connection: close 次に、Kuberentes cluster の中から Service を通した request をしてみましょう。 curl が入っている docker image として radial/busyboxplus:curl を使うことにします（注: radial/busyboxplus:curl は https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/ の中でも利用されてるので、安全な image だと判断して利用してます）。\nすると、以下のように Kubernetes cluster 内の pod から $ curl http://dumper.default で 「default namespace の dumper service へ HTTP request」をして、ちゃんと response が返る事を確認出来ました！\n$ kubectl run --rm -it busybox --image=radial/busyboxplus:curl If you don\u0026#39;t see a command prompt, try pressing enter. [ root@busybox:/ ]$ curl http://dumper.default Hello, dumper! pod の log を見てみると、ちゃんと上記 request が pod に到達していることも確認できます。\n$ kubectl logs dumper-6465654fdc-qn729 . . . 2020/12/30 12:12:00 GET / HTTP/1.1 2020/12/30 12:12:00 Host: dumper.default 2020/12/30 12:12:00 User-Agent: curl/7.35.0 2020/12/30 12:12:00 Accept: */* 簡単にではありますが、Kubernetes cluster としての動作が確認できました。\nDocker container として動く Node container の中を見てみる 次に、kind の動作をもう少し深掘りしてみます。具体的には、Deep Dive: Kind - KubeCon + CloudNativeCon North America 2019 の中で紹介されていた「Node container の動作」をもう少し見てみます。\nまず、Kubernetes cluster を一度削除して作り直すことにします。これは、「作成されたばかりの状態の Kubernetes cluster」で実験するための操作です（特に気にならない人は不要です）。\n$ kind delete cluster Deleting cluster \u0026#34;kind\u0026#34; ... $ kind create cluster Creating cluster \u0026#34;kind\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.19.1) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \u0026#34;kind-kind\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-kind Not sure what to do next? 😅 Check out https://kind.sigs.k8s.io/docs/user/quick-start/ kind create cluster 4.62s user 2.53s system 19% cpu 36.518 total 次に、$ kind create cluster を実行した後の状態で $ docker ps すると、kindest/node:v1.19.1 という image の container が起動している事が分かります。これが、kind が利用する「Node container」のようです。\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 45a383679dd2 kindest/node:v1.19.1 \u0026#34;/usr/local/bin/entr…\u0026#34; About a minute ago Up 59 seconds 127.0.0.1:56856-\u0026gt;6443/tcp kind-control-plane $ docker exec して Node container の中をみてみましょう。$ ps fax で process 一覧を見てみると、Deep Dive: Kind - KubeCon + CloudNativeCon North America 2019 の中で説明されていた通り、Kubernetes の動作を支える様々な process が起動している事が分かります。containerd や kubelet など一部を除くと、そのほかの process は /usr/local/bin/containerd-shim-runc-v2 経由で起動している（= container として起動している）ことも分かります。\n$ docker exec -it 45a383679dd2 bash root@kind-control-plane:/# ps fax PID TTY STAT TIME COMMAND 2060 pts/1 Ss 0:00 bash 2189 pts/1 R+ 0:00 \\_ ps fax 1 ? Ss 0:00 /sbin/init 124 ? S\u0026lt;s 0:00 /lib/systemd/systemd-journald 135 ? Ssl 0:11 /usr/local/bin/containerd 310 ? Sl 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 1c426469eb3ae09b744f1c116e6798c65886e218271dfa105fba747b4bfde0d3 -address /run/containerd/containerd.soc 405 ? Ss 0:00 \\_ /pause 502 ? Ssl 0:06 \\_ kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --ku 311 ? Sl 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 23d54713b4401fb309725273c78961ea5d7f3a5be157d2a139813b9b9611e220 -address /run/containerd/containerd.soc 418 ? Ss 0:00 \\_ /pause 620 ? Ssl 0:20 \\_ etcd --advertise-client-urls=https://172.19.0.2:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --initial-a 318 ? Sl 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id a24a84c14111721de85b657f2bb0b89db44d87e97452ef86690060a0f0fcd3bc -address /run/containerd/containerd.soc 425 ? Ss 0:00 \\_ /pause 563 ? Ssl 1:08 \\_ kube-apiserver --advertise-address=172.19.0.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admissi 373 ? Sl 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 8a6a56e83a590f4958dbd3262e4b0adbe36acad229ebcb54ef13c657f39c2c0a -address /run/containerd/containerd.soc 433 ? Ss 0:00 \\_ /pause 548 ? Ssl 0:24 \\_ kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes 667 ? Ssl 0:25 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cont 797 ? Sl 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 841020b0c947c1a8c2047a542268bf3dd91f8fb6b15cfc0a993dc61c0769e660 -address /run/containerd/containerd.soc 819 ? Ss 0:00 \\_ /pause 898 ? Ssl 0:00 \\_ /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=kind-control-plane 833 ? Sl 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 6171f628b9c0658a1983b198160fe76b53eeb4118aa8a0c71491ff5516453268 -address /run/containerd/containerd.soc 864 ? Ss 0:00 \\_ /pause 948 ? Ssl 0:00 \\_ /bin/kindnetd 1110 ? Sl 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 1fd32fb832a5e3782d1f39b26b99a30de8872a0a69600a77db3631f274eeb819 -address /run/containerd/containerd.soc 1153 ? Ss 0:00 \\_ /pause 1226 ? Ssl 0:03 \\_ /coredns -conf /etc/coredns/Corefile 1112 ? Sl 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 725b634f7fc5d93ebfb1e63afb8aabcd936e6297ed7ab552e314e1cc90efeec5 -address /run/containerd/containerd.soc 1160 ? Ss 0:00 \\_ /pause 1229 ? Ssl 0:01 \\_ local-path-provisioner --debug start --helper-image k8s.gcr.io/build-image/debian-base:v2.1.0 --config /etc/config/config.json 1350 ? Sl 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 9d07db434b356d9fddb2ae31715adc1209406a604b7068560f379832f5d79ba2 -address /run/containerd/containerd.soc 1373 ? Ss 0:00 \\_ /pause 1404 ? Ssl 0:03 \\_ /coredns -conf /etc/coredns/Corefile containerd で起動している container は、containerd の CLI tool である ctr で管理する事が出来るはずです。少し見てみましょう。\nまず、namespace 一覧を見てみると k8s.io という名前の namespace が見つかります（注: この namespace は kuberentes の namespace とは無関係で、「containerd が container を管理する際に利用する namespace 機能」のはずです）。\nroot@kind-control-plane:/# ctr namespaces ls NAME LABELS k8s.io 次に、この k8s.io namespace 内の container 一覧を $ ctr --namespace k8s.io containers ls で見てみると、予想通り「Kuberentes cluster の動作に利用される container 一覧」をみる事が出来ました。\nroot@kind-control-plane:/# ctr --namespace k8s.io containers ls CONTAINER IMAGE RUNTIME 0e7fc11f71638b86f8fd41f046101ebcb16b48976f06826add2d35df4e2ccc10 k8s.gcr.io/kube-controller-manager:v1.19.1 io.containerd.runc.v2 114d8f7f34ebc00c00236d7a111961193a6fa300dc90a4114385134f9eeda412 k8s.gcr.io/kube-proxy:v1.19.1 io.containerd.runc.v2 1c426469eb3ae09b744f1c116e6798c65886e218271dfa105fba747b4bfde0d3 k8s.gcr.io/pause:3.3 io.containerd.runc.v2 1fd32fb832a5e3782d1f39b26b99a30de8872a0a69600a77db3631f274eeb819 k8s.gcr.io/pause:3.3 io.containerd.runc.v2 2299e2a7b2b7afbb0789b30a4d7f4e57220a650f7368c04439e91c72e5049356 k8s.gcr.io/kube-apiserver:v1.19.1 io.containerd.runc.v2 23d54713b4401fb309725273c78961ea5d7f3a5be157d2a139813b9b9611e220 k8s.gcr.io/pause:3.3 io.containerd.runc.v2 35ab22ae753e043553188810bddfec43aa048d8c93f1ca38cf868ee31dbe06fc k8s.gcr.io/coredns:1.7.0 io.containerd.runc.v2 6171f628b9c0658a1983b198160fe76b53eeb4118aa8a0c71491ff5516453268 k8s.gcr.io/pause:3.3 io.containerd.runc.v2 6212a3ea51397a8491a8defb188faa1c9afb4b678a8fa102ab15ba8c78f98aa2 sha256:0369cf4303ffdb467dc219990960a9baa8512a54b0ad9283eaf55bd6c0adb934 io.containerd.runc.v2 624a9cf197014dcdbf0be5ddd68995566d14ceab00c8ca18fd51eb35cfe999cb k8s.gcr.io/kube-scheduler:v1.19.1 io.containerd.runc.v2 62d494fe1a94a396494ecd30cfa8538db2e1d2055fedac216d19fd21332d3841 sha256:b77790820d01598b2c56f823fa489e3f56be2cb5d6f7dd9eecd68a1995b89c13 io.containerd.runc.v2 725b634f7fc5d93ebfb1e63afb8aabcd936e6297ed7ab552e314e1cc90efeec5 k8s.gcr.io/pause:3.3 io.containerd.runc.v2 841020b0c947c1a8c2047a542268bf3dd91f8fb6b15cfc0a993dc61c0769e660 k8s.gcr.io/pause:3.3 io.containerd.runc.v2 8a6a56e83a590f4958dbd3262e4b0adbe36acad229ebcb54ef13c657f39c2c0a k8s.gcr.io/pause:3.3 io.containerd.runc.v2 9d07db434b356d9fddb2ae31715adc1209406a604b7068560f379832f5d79ba2 k8s.gcr.io/pause:3.3 io.containerd.runc.v2 a24a84c14111721de85b657f2bb0b89db44d87e97452ef86690060a0f0fcd3bc k8s.gcr.io/pause:3.3 io.containerd.runc.v2 b7775d2582ea9fdd481cf308d9c8bafa28fffbdaa2c8c0bad3377a9254876a59 k8s.gcr.io/coredns:1.7.0 io.containerd.runc.v2 ecbad3d6f6f5321e46b0d3ac395cb25227b42cfc04b1cec5a2b659fe45fab6cc sha256:e422121c9c5f97623245b7e600eeb5e223ee623f21fa04da985ae71057d8d70b io.containerd.runc.v2 このように、「Node container の中で containerd を動かし、その containerd 経由で Kuberentes cluster に必要な container を動かす」という形で kind は動作するようです。Deep Dive: Kind - KubeCon + CloudNativeCon North America 2019 の中で説明されていた事ではありますが、改めてその動作を確認できました。\nkind と minikube の使い分けについて さて、ここまでは kind というツールの機能について説明してきました。一方で、「local で Kubernetes を動かすツール」としては他に minikube も存在します。これらの使い分けはどうするのが良いでしょうか？\nkind vs minikube で検索すると、この2つの使い分けについて言及している記事がいくつか見つかります。例えば https://brennerm.github.io/posts/minikube-vs-kind-vs-k3s.html という記事では、以下のようにそれぞれの特徴が述べられています。\nminikube VM で Kubernetes を起動する $ minikube dashboard 機能や minikube の addon system が有用 kind Docker container で Kubernetes を起動する $ kind load docker-image \u0026lt;my-custom-image\u0026gt;:\u0026lt;tag\u0026gt; を実行する事で local で build した image を container registry へ push する事なく Kubernetes cluster から利用する事が可能 kind について言えば、$ kind load docker-image の機能はかなり強力で、例えば custom controller のような「Kubernetes の中で動かしながら開発を進めたいもの」においては極めて有用です。実際、こういった側面があるためか、kubebuilder の Tutorial の Quick Start では「local で Kuberentes cluster を動かす選択肢」として kind が紹介されていたりします。\nYou’ll need a Kubernetes cluster to run against. You can use KIND to get a local cluster for testing, or run against a remote cluster.\ncf. https://book.kubebuilder.io/quick-start.html#test-it-out\nまた、「VM よりも Docker container の方が扱いに慣れてる」などのケースでも kind の利用にはメリットがありそうだと個人的には感じました。\nまとめ kind の内部構造や使い方についてざっと眺めてみました。\n「Kuberentes を Docker container の中で動かす」という言葉だけを聞くと突拍子も無いアイディアに聞こえますが、kubelet や containerd, その他の Kubernetes cluster に必要な component を \u0026ldquo;Node\u0026rdquo; image の中にまとめて配布してると思えば確かに自然ですし、実際にちゃんと動作することも確認できました。\nKuberentes の CI で使われている限り、これからも継続してメンテナンスされていきそうなのも良い点です。「30s で Kubernetes cluster を起動して気軽に動かせるツール」として、とても有用なものだと言えそうです。\nなお、今回は「kind の Node container で動く container 一覧を眺めた」だけで、1つ1つの container の動きについては特に言及しませんでした。そのほとんどは「Kubernetes cluster に共通で必要な component」のはずですが、kindnetd は kind におけるデフォルトの CNI plugin である kindnet の daemon だそうです。\nDeep Dive: Kind - KubeCon + CloudNativeCon North America 2019 ではこの kindnet を含めて、このブログで言及してない様々な事を説明してるので、さらに詳細が気になる方はぜひ動画の方もみてみてください。また、自分も一部しか読んでませんが、kind の document (https://kind.sigs.k8s.io/) も理解を深める上でとても有用だと思います。\n参考情報 Kubernetes tools: https://kubernetes.io/docs/tasks/tools/ kind: https://github.com/kubernetes-sigs/kind kind quick start: https://kind.sigs.k8s.io/docs/user/quick-start Deep Dive: Kind - KubeCon + CloudNativeCon North America 2019: https://www.youtube.com/watch?v=tT-GiZAr6eQ おまけ: 既に node image を pull 済の場合に Kuberentes cluster 作成にかかる時間について 最初に kind を動かしてみた際に、「kindest/node の docker pull 部分で時間がかかる」と書きました。そこで、「既に node image を pull 済みの場合」についても、試しに計測してみましょう。\nまず、先ほど作成した cluster を削除します。\n$ kind delete cluster Deleting cluster \u0026#34;kind\u0026#34; ... この状態でも、kindest/node docker image は残っている事が確認できます。\n$ docker images | grep kindest kindest/node \u0026lt;none\u0026gt; 37ddbc9063d2 3 months ago 1.33GB 次に、この状態で再度 $ kind create cluster を実行してみます。\n$ kind create cluster Creating cluster \u0026#34;kind\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.19.1) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \u0026#34;kind-kind\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! 👋 kind create cluster 4.53s user 2.37s system 21% cpu 32.535 total 今回は、上記のように 30s 程度で Kubernetes cluster が起動することを確認できました！🎉 Node image の pull が無ければ、Kuberentes cluster をとても素早く作成出来る事が分かりますね！\nおまけ 2: deployment を apply した状態で Node container の中を見てみる deployment を apply した状態で、Node container の中を見てみましょう。\n$ kubectl apply -f dumper-deployment.yaml deployment.apps/dumper created この状態だと、（当然ではありますが）apply した内容の pod に相当する container が起動している様子を確認できます。containerd の動作を感じる事が出来て、とても良いですね！\n$ docker exec -it 7ff1d05ef709 bash root@kind-control-plane:/# ps fax PID TTY STAT TIME COMMAND . . . 1729 ? Sl 0:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 34794c98df201080b5b22bcef08805e03748023c35f0deec77f962ff301a6835 -address /run/containerd/containerd.sock 1752 ? Ss 0:00 \\_ /pause 1879 ? Ssl 0:00 \\_ /app/dumper root@kind-control-plane:/# ctr --namespace k8s.io images ls | grep dumper docker.io/south37/dumper:latest application/vnd.docker.distribution.manifest.v2+json sha256:5efcf15fbd3439b2c2fff2415957933b45b9531401526c026c41219aed15701c 290.0 MiB linux/amd64 io.cri-containerd.image=managed docker.io/south37/dumper@sha256:5efcf15fbd3439b2c2fff2415957933b45b9531401526c026c41219aed15701c application/vnd.docker.distribution.manifest.v2+json sha256:5efcf15fbd3439b2c2fff2415957933b45b9531401526c026c41219aed15701c 290.0 MiB linux/amd64 io.cri-containerd.image=managed root@kind-control-plane:/# ctr --namespace k8s.io containers ls | grep dumper ab80cafc653433c2b74713b85679797b4ffad5ae54eed733bb40d41af7bb9f43 docker.io/south37/dumper:latest io.containerd.runc.v2 おまけ 3: $ kind load docker-image の実装について kind の強力な機能である $ kind load docker-image 機能がどう実現されているのか気になったので、少しコードを読んでみました（対象は kind の v0.9.0 tag のコードです）。\nまず、$ kind load コマンド自体は https://github.com/kubernetes-sigs/kind/blob/v0.9.0/pkg/cmd/kind/load/load.go で実装されていて、その中の $ kind load docker-image サブコマンドは https://github.com/kubernetes-sigs/kind/blob/v0.9.0/pkg/cmd/kind/load/docker-image/docker-image.go で実装されてるようです。さらにコードを読み進めると、nodeutils package の LoadImageArchive という関数にたどり着きます。\nimport ( ... dockerimage \u0026#34;sigs.k8s.io/kind/pkg/cmd/kind/load/docker-image\u0026#34; ... ) // NewCommand returns a new cobra.Command for get func NewCommand(logger log.Logger, streams cmd.IOStreams) *cobra.Command { cmd := \u0026amp;cobra.Command{ Args: cobra.NoArgs, Use: \u0026#34;load\u0026#34;, Short: \u0026#34;Loads images into nodes\u0026#34;, Long: \u0026#34;Loads images into node from an archive or image on host\u0026#34;, } // add subcommands cmd.AddCommand(dockerimage.NewCommand(logger, streams)) cmd.AddCommand(imagearchive.NewCommand(logger, streams)) return cmd } cf. https://github.com/kubernetes-sigs/kind/blob/v0.9.0/pkg/cmd/kind/load/load.go#L38\n// Load the image on the selected nodes fns := []func() error{} for _, selectedNode := range selectedNodes { selectedNode := selectedNode // capture loop variable fns = append(fns, func() error { return loadImage(imageTarPath, selectedNode) }) } cf. https://github.com/kubernetes-sigs/kind/blob/v0.9.0/pkg/cmd/kind/load/docker-image/docker-image.go#L149-L156\n// loads an image tarball onto a node func loadImage(imageTarName string, node nodes.Node) error { f, err := os.Open(imageTarName) if err != nil { return errors.Wrap(err, \u0026#34;failed to open image\u0026#34;) } defer f.Close() return nodeutils.LoadImageArchive(node, f) } cf. https://github.com/kubernetes-sigs/kind/blob/v0.9.0/pkg/cmd/kind/load/docker-image/docker-image.go#L162-L170\nnodeutils.LoadImageArchive は以下のような実装になっていて、「Node container の中で $ ctr コマンドを呼び出して、container image の load を行う」ようです。\n// LoadImageArchive loads image onto the node, where image is a Reader over an image archive func LoadImageArchive(n nodes.Node, image io.Reader) error { cmd := n.Command(\u0026#34;ctr\u0026#34;, \u0026#34;--namespace=k8s.io\u0026#34;, \u0026#34;images\u0026#34;, \u0026#34;import\u0026#34;, \u0026#34;-\u0026#34;).SetStdin(image) if err := cmd.Run(); err != nil { return errors.Wrap(err, \u0026#34;failed to load image\u0026#34;) } return nil } cf. https://github.com/kubernetes-sigs/kind/blob/v0.9.0/pkg/cluster/nodeutils/util.go#L77-L84\nnodeutils.LoadImageArchive は「1 つ 1 つの Node container に対して loop を回して実行してる」ようです。つまり、魔法のように見えた $ kind load docker-image の機能は、「それぞれの Node container の中で $ ctr images import を実行して、container image を import する」事で実現しているようです。面白いですね！\n","permalink":"https://south37.link/posts/20201230-kind/","summary":"kuKubernetes の Install Tools というページでは、kubernetes を local で動かすための tool として kind が紹介されています。今日はこの kind について内部構造及び使い方を見てみます。\nkind とは kind は「Docker container の中で Kubernetes を動かすことが出来るツール」です。kind という命名は「Kubernetes in Docker」から来ていて、K-in-D という頭文字をとったものになっています。\nkind については KubeCon + CloudNativeCon で何度か紹介されているようです。例えば KubeCon + CloudNativeCon North America 2019 における以下の \u0026ldquo;Deep Dive: Kind\u0026rdquo; というトーク では、「kind とは何か？」について内部実装など含めて紹介されています。\n上記のトークについて簡単に summary を書くと、kind は以下のようなものとして紹介されています。\nDocker container として Node（をシミュレートする container）を動かし、その中で Kubernetes を動かすツール Node image の中に、Kubernetes を動かすために必要な全てを詰める kubelet kubeadm docker systemd core images (Kuberentes にとって重要な container image) etcd coredns pause kube-apiserver etc.","title":"kind (Kuberenetes in Docker) に deep dive してみる"},{"content":"BigQuery（正確にはそのクエリエンジンである Dremel）の内部実装の変遷をまとめた以下のブログポストおよび論文を読みました。\nhttps://medium.com/google-cloud-jp/i-read-dremel-a-decade-of-interactive-sql-analysis-at-web-scale-ca2a015a522a\nhttps://research.google/pubs/pub49489/\nとても面白い内容で、Twitter にメモをポストしたのですが、後で参照しやすいようにブログにも同じ内容を載せておきます。\nBigQuery（正確にはそのクエリエンジンである Dremel）の内部実装の変遷をまとめた論文の解説ブログ。面白かった。https://t.co/s9UaH4MjBS\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 解説対象の論文は \u0026quot;Dremel: A Decade of Interactive SQL Analysis at Web Scale\u0026quot;。上記ブログポストでは 3 章の Disaggregation と 5 章の Serverless Computing をピックアップして解説していたが、より網羅的に知りたい場合 \u0026amp; 原文を読みたい場合はこちらを読むと良い。https://t.co/XolZ7fMXFY\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 以下、雑なメモ。まず前提として 2010 年の Dremel についての最初の論文である \u0026quot;Dremel: Interactive Analysis of Web-Scale Datasets\u0026quot; の 6 章 Query Execution は目を通しておくと良い。Dremel が Query をどう実行するかが説明されている。https://t.co/A98EC7oBqE\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 Dremel は「巨大なデータに対しての解析 query」である入力 query を「小さなデータに対しての解析 query」の集合に分解する。分解は 2010 年時点では tree の形で行われて、leaf query は storage からデータを読み取る形で実行される。中間 node は「child から集めたデータに対する query」を実行。\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 中間 node の挙動は「query の書き換え」として実現される。具体的には、\u0026quot;SELECT A, COUNT(B) FROM T GROUP BY A\u0026quot; は \u0026quot;SELECT A, SUM(c) FROM (R1 UNION ALL ... Rn) GROUP BY A\u0026quot; へと書き換えられる。Ri は child node で query を実行した結果となっている。\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 上記の Query Execution の前提がある中で、Dremel が 2010 年時点から進化したポイントがいくつかある。1つは、storage として local storage ではなく GFS （やその後継の Colossus）を利用するようになったこと（= Disaggregated storage）。\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 元々 Dremel の server（特に leaf server）は local storage を利用していた。それが GFS を利用するようになり、Dremel の信頼性向上やメンテナンス性向上につながった（fully-managed な GFS を利用することで robustness が向上したり、local disk へのデータロードが不要になったりしたらしい）\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 Dremel は join, aggregation, analytic operation のために shuffle という機構を持つが、それも進化したらしい。これは以下の blog post でも説明されてる。shuffle は元々 Map Reduce で導入され、Hadoop, Spark など主要な分散データ処理システムの動作を支える重要な機構https://t.co/ycJwG0aTo4\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 2014 年に Dremel の shuffle は進化して、中間データを \u0026quot; distributed transient storage system\u0026quot; に持つようになった（= Disaggregated memory）。これにより、完全な in-memory での query 実行が実現されるようになったらしい。\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 新しい shuffle infrastructure は latency 削減や shuffle 対象のデータ量の向上、20%以上の resource cost 削減に繋がった（おそらく、resource utilization が高まったのだと思われる）とのこと。ちなみに、同じ shuffle infrastructure が Google Cloud Dataflow でも使われているらしい。\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 また、disaggregated memory shuffle system が利用されるようになったことで、query 実行の compute resource の割り当て（= scheduling）に柔軟性が生まれた。具体的には、shuffle 結果を checkpoint として、動的に worker を preempt 出来るようになったらしい（= Shuffle Persistence Layer）\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 また、2010年時点では \u0026quot;fixed execution tree\u0026quot; として実行されていた query が、スケジューラの進化（= \u0026quot;centralized scheduling\u0026quot;）と \u0026quot;shuffle persistence layer\u0026quot; のおかげで「DAG として表現された query plan に対して、柔軟に worker を割り当てる」という形で実行されるようになった。\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 まとめると、storage, memory など様々な resource が分離され、それによって compute resource の柔軟な割り当てが可能になった。performance, resource utilization の両面で劇的に進化しており、さらに改善が積み重なっている様子が感じ取られて面白い内容だった。\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 上記で言及してない部分で面白い箇所はいっぱいあるので、ぜひ論文をご一読ください！https://t.co/XolZ7fMXFY\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 ","permalink":"https://south37.link/posts/20201228-bigquery/","summary":"BigQuery（正確にはそのクエリエンジンである Dremel）の内部実装の変遷をまとめた以下のブログポストおよび論文を読みました。\nhttps://medium.com/google-cloud-jp/i-read-dremel-a-decade-of-interactive-sql-analysis-at-web-scale-ca2a015a522a\nhttps://research.google/pubs/pub49489/\nとても面白い内容で、Twitter にメモをポストしたのですが、後で参照しやすいようにブログにも同じ内容を載せておきます。\nBigQuery（正確にはそのクエリエンジンである Dremel）の内部実装の変遷をまとめた論文の解説ブログ。面白かった。https://t.co/s9UaH4MjBS\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 解説対象の論文は \u0026quot;Dremel: A Decade of Interactive SQL Analysis at Web Scale\u0026quot;。上記ブログポストでは 3 章の Disaggregation と 5 章の Serverless Computing をピックアップして解説していたが、より網羅的に知りたい場合 \u0026amp; 原文を読みたい場合はこちらを読むと良い。https://t.co/XolZ7fMXFY\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 以下、雑なメモ。まず前提として 2010 年の Dremel についての最初の論文である \u0026quot;Dremel: Interactive Analysis of Web-Scale Datasets\u0026quot; の 6 章 Query Execution は目を通しておくと良い。Dremel が Query をどう実行するかが説明されている。https://t.co/A98EC7oBqE\n\u0026mdash; south37/Nao Minami (@south37777) December 28, 2020 Dremel は「巨大なデータに対しての解析 query」である入力 query を「小さなデータに対しての解析 query」の集合に分解する。分解は 2010 年時点では tree の形で行われて、leaf query は storage からデータを読み取る形で実行される。中間 node は「child から集めたデータに対する query」を実行。","title":"BigQuery の内部実装の変遷について"},{"content":"コンテナ標準化の現状と Kubernetes の立ち位置について というブログではコンテナ標準の現状についてまとめてみました。\nまた、手を動かして学ぶコンテナ標準 - Container Image と Container Registry 編 というブログでは Container Image と Container Registry について手を動かして学んでみました。\nこのブログでは、runc, containerd などの Container Runtime について、実際に手を動かして学んでみたいと思います。\nなお、前回のブログ 同様、基本的にこのブログ内のコマンドは Linux で実行するものとします（自分は MacOS で Vagrant で Ubuntu VM を立てて実験してます）。\nrunc を動かしてみる runc は Low-Level Container Runtime と呼ばれるもので、OCI Runtime Specification に準拠した CLI tool となっています。実際に runc を動かしてみることで、Container Runtime に対して理解を深めてみましょう。\nまず runc の install ですが、自分が試している ubuntu-20.04 では apt で install することができます。\nvagrant@vagrant:~$ sudo apt update -y vagrant@vagrant:~$ sudo apt install -y runc vagrant@vagrant:~$ which runc /usr/sbin/runc help を見てみると以下のような内容になっていて、container を操作するために必要な各種 subcommand が存在することがわかります。実は、これらの subcommand によって OCI Runtime Specification の Runtime and Lifecycle で定義された「Operations」 の機能が提供されています。ただし、見比べてみると分かりますが runc 自体はよりリッチな機能を提供しているようです。\nvagrant@vagrant:~$ runc --help NAME: runc - Open Container Initiative runtime runc is a command line client for running applications packaged according to the Open Container Initiative (OCI) format and is a compliant implementation of the Open Container Initiative specification. runc integrates well with existing process supervisors to provide a production container runtime environment for applications. It can be used with your existing process monitoring tools and the container will be spawned as a direct child of the process supervisor. Containers are configured using bundles. A bundle for a container is a directory that includes a specification file named \u0026#34;config.json\u0026#34; and a root filesystem. The root filesystem contains the contents of the container. To start a new instance of a container: # runc run [ -b bundle ] \u0026lt;container-id\u0026gt; Where \u0026#34;\u0026lt;container-id\u0026gt;\u0026#34; is your name for the instance of the container that you are starting. The name you provide for the container instance must be unique on your host. Providing the bundle directory using \u0026#34;-b\u0026#34; is optional. The default value for \u0026#34;bundle\u0026#34; is the current directory. USAGE: runc [global options] command [command options] [arguments...] VERSION: spec: 1.0.1-dev COMMANDS: checkpoint checkpoint a running container create create a container delete delete any resources held by the container often used with detached container events display container events such as OOM notifications, cpu, memory, and IO usage statistics exec execute new process inside the container init initialize the namespaces and launch the process (do not call it outside of runc) kill kill sends the specified signal (default: SIGTERM) to the container\u0026#39;s init process list lists containers started by runc with the given root pause pause suspends all processes inside the container ps ps displays the processes running inside a container restore restore a container from a previous checkpoint resume resumes all processes that have been previously paused run create and run a container spec create a new specification file start executes the user defined process in a created container state output the state of a container update update container resource constraints help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --debug enable debug output for logging --log value set the log file path where internal debug information is written --log-format value set the format used by logs (\u0026#39;text\u0026#39; (default), or \u0026#39;json\u0026#39;) (default: \u0026#34;text\u0026#34;) --root value root directory for storage of container state (this should be located in tmpfs) (default: \u0026#34;/run/user/1000/runc\u0026#34;) --criu value path to the criu binary used for checkpoint and restore (default: \u0026#34;criu\u0026#34;) --systemd-cgroup enable systemd cgroup support, expects cgroupsPath to be of form \u0026#34;slice:prefix:name\u0026#34; for e.g. \u0026#34;system.slice:runc:434234\u0026#34; --rootless value ignore cgroup permission errors (\u0026#39;true\u0026#39;, \u0026#39;false\u0026#39;, or \u0026#39;auto\u0026#39;) (default: \u0026#34;auto\u0026#34;) --help, -h show help --version, -v print the version 実際に runc を利用して container を起動してみましょう。runc の README の Using runc というセクションがぴったりの題材なので、これを参考にしてみます。\nOCI Runtime Speicification の内容を見てもらうと分かるのですが、runc のような Low-Level Container Runtime は Filesystem Bundle と呼ばれる「Container を起動するのに利用される configuration file と、container にとっての root filesystem となる directory の組み合わせ」を必要とします。まずはこの Filesystem Bundle を用意します。\nFilesystem Bundle は config.json と呼ばれる configuration file と、その config.json の中の root.path で path を指定される 「container の root filesystem となる directory」から構成されます。それぞれ順番に作成していきます。\nまずは Filesystem Bundle を格納する directory として mycontainer という directory を作っておきます。\nvagrant@vagrant:~$ mkdir mycontainer vagrant@vagrant:~$ cd mycontainer/ 次に、config.json についてですが、 $ runc spec を実行すると自動でデフォルト設定の config.json が作成されます。今回はこれを使うことにしましょう。\nvagrant@vagrant:~/mycontainer$ runc spec vagrant@vagrant:~/mycontainer$ ls config.json デフォルト設定の config.json は以下のような内容になっています。実行時のコマンドや環境変数、root.path などに加えて、namespace や volume mount, capability の設定なども記載されています。\nvagrant@vagrant:~/mycontainer$ cat config.json { \u0026#34;ociVersion\u0026#34;: \u0026#34;1.0.1-dev\u0026#34;, \u0026#34;process\u0026#34;: { \u0026#34;terminal\u0026#34;: true, \u0026#34;user\u0026#34;: { \u0026#34;uid\u0026#34;: 0, \u0026#34;gid\u0026#34;: 0 }, \u0026#34;args\u0026#34;: [ \u0026#34;sh\u0026#34; ], \u0026#34;env\u0026#34;: [ \u0026#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;TERM=xterm\u0026#34; ], \u0026#34;cwd\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;bounding\u0026#34;: [ \u0026#34;CAP_AUDIT_WRITE\u0026#34;, \u0026#34;CAP_KILL\u0026#34;, \u0026#34;CAP_NET_BIND_SERVICE\u0026#34; ], \u0026#34;effective\u0026#34;: [ \u0026#34;CAP_AUDIT_WRITE\u0026#34;, \u0026#34;CAP_KILL\u0026#34;, \u0026#34;CAP_NET_BIND_SERVICE\u0026#34; ], \u0026#34;inheritable\u0026#34;: [ \u0026#34;CAP_AUDIT_WRITE\u0026#34;, \u0026#34;CAP_KILL\u0026#34;, \u0026#34;CAP_NET_BIND_SERVICE\u0026#34; ], \u0026#34;permitted\u0026#34;: [ \u0026#34;CAP_AUDIT_WRITE\u0026#34;, \u0026#34;CAP_KILL\u0026#34;, \u0026#34;CAP_NET_BIND_SERVICE\u0026#34; ], \u0026#34;ambient\u0026#34;: [ \u0026#34;CAP_AUDIT_WRITE\u0026#34;, \u0026#34;CAP_KILL\u0026#34;, \u0026#34;CAP_NET_BIND_SERVICE\u0026#34; ] }, \u0026#34;rlimits\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;RLIMIT_NOFILE\u0026#34;, \u0026#34;hard\u0026#34;: 1024, \u0026#34;soft\u0026#34;: 1024 } ], \u0026#34;noNewPrivileges\u0026#34;: true }, \u0026#34;root\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;rootfs\u0026#34;, \u0026#34;readonly\u0026#34;: true }, \u0026#34;hostname\u0026#34;: \u0026#34;runc\u0026#34;, \u0026#34;mounts\u0026#34;: [ { \u0026#34;destination\u0026#34;: \u0026#34;/proc\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;proc\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;proc\u0026#34; }, { \u0026#34;destination\u0026#34;: \u0026#34;/dev\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;tmpfs\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;tmpfs\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;strictatime\u0026#34;, \u0026#34;mode=755\u0026#34;, \u0026#34;size=65536k\u0026#34; ] }, { \u0026#34;destination\u0026#34;: \u0026#34;/dev/pts\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;devpts\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;devpts\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;noexec\u0026#34;, \u0026#34;newinstance\u0026#34;, \u0026#34;ptmxmode=0666\u0026#34;, \u0026#34;mode=0620\u0026#34;, \u0026#34;gid=5\u0026#34; ] }, { \u0026#34;destination\u0026#34;: \u0026#34;/dev/shm\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;tmpfs\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;shm\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;noexec\u0026#34;, \u0026#34;nodev\u0026#34;, \u0026#34;mode=1777\u0026#34;, \u0026#34;size=65536k\u0026#34; ] }, { \u0026#34;destination\u0026#34;: \u0026#34;/dev/mqueue\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;mqueue\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;mqueue\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;noexec\u0026#34;, \u0026#34;nodev\u0026#34; ] }, { \u0026#34;destination\u0026#34;: \u0026#34;/sys\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;sysfs\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;sysfs\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;noexec\u0026#34;, \u0026#34;nodev\u0026#34;, \u0026#34;ro\u0026#34; ] }, { \u0026#34;destination\u0026#34;: \u0026#34;/sys/fs/cgroup\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cgroup\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;cgroup\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;noexec\u0026#34;, \u0026#34;nodev\u0026#34;, \u0026#34;relatime\u0026#34;, \u0026#34;ro\u0026#34; ] } ], \u0026#34;linux\u0026#34;: { \u0026#34;resources\u0026#34;: { \u0026#34;devices\u0026#34;: [ { \u0026#34;allow\u0026#34;: false, \u0026#34;access\u0026#34;: \u0026#34;rwm\u0026#34; } ] }, \u0026#34;namespaces\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;pid\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;network\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;ipc\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;uts\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;mount\u0026#34; } ], \u0026#34;maskedPaths\u0026#34;: [ \u0026#34;/proc/acpi\u0026#34;, \u0026#34;/proc/asound\u0026#34;, \u0026#34;/proc/kcore\u0026#34;, \u0026#34;/proc/keys\u0026#34;, \u0026#34;/proc/latency_stats\u0026#34;, \u0026#34;/proc/timer_list\u0026#34;, \u0026#34;/proc/timer_stats\u0026#34;, \u0026#34;/proc/sched_debug\u0026#34;, \u0026#34;/sys/firmware\u0026#34;, \u0026#34;/proc/scsi\u0026#34; ], \u0026#34;readonlyPaths\u0026#34;: [ \u0026#34;/proc/bus\u0026#34;, \u0026#34;/proc/fs\u0026#34;, \u0026#34;/proc/irq\u0026#34;, \u0026#34;/proc/sys\u0026#34;, \u0026#34;/proc/sysrq-trigger\u0026#34; ] } 次に、container の root filesystem となる directory を用意します。今回は楽をするために、「$ docker export の機能を利用する」ことにします（自分は vagrant で synced_folder した directory で docker export しました）。原理的には、どんな手段で用意しても良いはずです。\n$ mkdir rootfs $ docker export $(docker create busybox) | tar -C rootfs -xvf - 上記コマンドまで実行が終わると、mycontainer directory の中は以下のような状態になっているはずです。\nvagrant@vagrant:~/mycontainer$ ls config.json rootfs この状態で $ sudo runc run containerid を実行すると、container が起動します。\nvagrant@vagrant:~/oci-playground/runc-playground/mycontainer2$ sudo runc run containerid / # 上記で作成した config.json はデフォルトで「sh コマンドを実行する」という設定になっているので、sh が起動しています。\nそのため、一度 exit してから config.json を例えば「ls を実行するもの」に書き換えると、ls が container 内で実行されます。\nvagrant@vagrant:~/oci-playground/runc-playground/mycontainer2$ vim config.json # ここで以下のように変更 \u0026#34;args\u0026#34;: [ - \u0026#34;sh\u0026#34; + \u0026#34;ls\u0026#34; ], vagrant@vagrant:~/oci-playground/runc-playground/mycontainer2$ sudo runc run containerid bin dev etc home proc root sys tmp usr var ということで、runc を利用して無事に container を起動することができました！\nconfig.json + container の root filesystem になる directory という「Filesystem Bundle」を用意さえすれば、「container の起動が簡単に出来る」ということが分かったかと思います。なお、これは OCI Runtime Specification で定められている挙動であるため、この仕様を満たしてさえいれば「runc 以外の Low-Level Container Runtime」においても同様のことができるはずです。\n上記の例では runc run を利用しましたが、その他のオペレーション（create, start, delete など）を利用することでより細かな制御も可能です。詳細が気になる方は、ぜひご自分でも試してみてください。\ncontainerd を動かしてみる Low-Level Container Runtime の runc の次は、High-Level Container Runtime である containerd を動かしてみましょう。runc は「container を起動する」という部分は実行してくれましたが、それ以外の「Container Image の Pull、Container Image から Filesystem Bundle への変換」などはやってくれませんでした。実は、この部分の機能を提供してくれてるのが containerd です。\ncontainerd は、daemon として常駐して、client からの request を受け付けて動作するという振る舞いになっています。containerd の動作のイメージを掴むために、Getting started というページに従って containerd を利用してみましょう。\nまずは、containerd downloads の Installing binaries に従って containerd の binary を download します。そして、その binary を /usr/local/bin など PATH の通った場所に配置します。\nvagrant@vagrant:~$ wget https://github.com/containerd/containerd/releases/download/v1.4.3/containerd-1.4.3-linux-amd64.tar.gz vagrant@vagrant:~$ tar xvf containerd-1.4.3-linux-amd64.tar.gz vagrant@vagrant:~$ sudo mv bin/* /usr/local/bin/ これで、containerd がコマンドとして利用できるようになります。\nvagrant@vagrant:~$ which containerd /usr/local/bin/containerd vagrant@vagrant:~$ containerd --help NAME: containerd - __ _ __ _________ ____ / /_____ _(_)___ ___ _________/ / / ___/ __ \\/ __ \\/ __/ __ `/ / __ \\/ _ \\/ ___/ __ / / /__/ /_/ / / / / /_/ /_/ / / / / / __/ / / /_/ / \\___/\\____/_/ /_/\\__/\\__,_/_/_/ /_/\\___/_/ \\__,_/ high performance container runtime USAGE: containerd [global options] command [command options] [arguments...] VERSION: v1.4.3 DESCRIPTION: containerd is a high performance container runtime whose daemon can be started by using this command. If none of the *config*, *publish*, or *help* commands are specified, the default action of the **containerd** command is to start the containerd daemon in the foreground. A default configuration is used if no TOML configuration is specified or located at the default file location. The *containerd config* command can be used to generate the default configuration for containerd. The output of that command can be used and modified as necessary as a custom configuration. COMMANDS: config information on the containerd config publish binary to publish events to containerd oci-hook provides a base for OCI runtime hooks to allow arguments to be injected. help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --config value, -c value path to the configuration file (default: \u0026#34;/etc/containerd/config.toml\u0026#34;) --log-level value, -l value set the logging level [trace, debug, info, warn, error, fatal, panic] --address value, -a value address for containerd\u0026#39;s GRPC server --root value containerd root directory --state value containerd state directory --help, -h show help --version, -v print the version ただし、前述したように containerd は daemon として動作する必要があるので、これだけではまだ利用できません。\nGetting started を見ると、どうやら containerd のソースコードに同梱されている containerd.service に言及しています。これを利用して、systemd で containerd を起動することにしてみましょう。\nvagrant@vagrant:~$ sudo apt install -y unzip # unzip を install しておく vagrant@vagrant:~$ wget https://github.com/containerd/containerd/archive/v1.4.3.zip # ソースコードを取得 vagrant@vagrant:~$ unzip v1.4.3.zip vagrant@vagrant:~$ ls containerd-1.4.3 v1.4.3.zip vagrant@vagrant:~$ ls containerd-1.4.3/containerd.service # systemd 向けの service file が存在 containerd-1.4.3/containerd.service vagrant@vagrant:~$ sudo cp containerd-1.4.3/containerd.service /etc/systemd/system/ vagrant@vagrant:~$ sudo systemctl enable containerd Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /etc/systemd/system/containerd.service. vagrant@vagrant:~$ sudo systemctl start containerd これで、無事 containerd が起動しました。\nvagrant@vagrant:~$ ps aux | grep containerd root 13444 1.6 4.5 1344964 45620 ? Ssl 16:39 0:00 /usr/local/bin/containerd vagrant 13455 0.0 0.0 9032 664 pts/0 S+ 16:39 0:00 grep --color=auto containerd Getting started の document には /etc/containerd/config.toml の存在も言及されているので、これも用意しておきます。これで準備 OK です。\nvagrant@vagrant:~$ sudo mkdir /etc/containerd vagrant@vagrant:~$ sudo vim /etc/containerd/config.toml # ここで containerd の設定ファイルである config.toml を作成 vagrant@vagrant:~$ sudo systemctl restart containerd # ここで、設定ファイルを読み込む形で containerd を再起動 # これが /etc/containerd/config.toml の内容 subreaper = true oom_score = -999 [debug] level = \u0026#34;debug\u0026#34; [metrics] address = \u0026#34;127.0.0.1:1338\u0026#34; [plugins.linux] runtime = \u0026#34;runc\u0026#34; shim_debug = true さて、containerd が daemon として動くようになりました。次は、containerd と通信するコードを書いてみましょう。Getting started を参考に、以下のような Go コードを書いてみます。\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/containerd/containerd\u0026#34; \u0026#34;github.com/containerd/containerd/cio\u0026#34; \u0026#34;github.com/containerd/containerd/namespaces\u0026#34; \u0026#34;github.com/containerd/containerd/oci\u0026#34; ) func main() { if err := redisExample(); err != nil { log.Fatal(err) } } func redisExample() error { // create a new client connected to the default socket path for containerd client, err := containerd.New(\u0026#34;/run/containerd/containerd.sock\u0026#34;) if err != nil { return err } defer client.Close() // create a new context with an \u0026#34;example\u0026#34; namespace ctx := namespaces.WithNamespace(context.Background(), \u0026#34;example\u0026#34;) // pull the redis image from DockerHub image, err := client.Pull(ctx, \u0026#34;docker.io/library/redis:alpine\u0026#34;, containerd.WithPullUnpack) if err != nil { return err } // create a container container, err := client.NewContainer( ctx, \u0026#34;redis-server\u0026#34;, containerd.WithImage(image), containerd.WithNewSnapshot(\u0026#34;redis-server-snapshot\u0026#34;, image), containerd.WithNewSpec(oci.WithImageConfig(image)), ) if err != nil { return err } defer container.Delete(ctx, containerd.WithSnapshotCleanup) // create a task from the container task, err := container.NewTask(ctx, cio.NewCreator(cio.WithStdio)) if err != nil { return err } defer task.Delete(ctx) // make sure we wait before calling start exitStatusC, err := task.Wait(ctx) if err != nil { fmt.Println(err) } // call start on the task to execute the redis server if err := task.Start(ctx); err != nil { return err } // sleep for a lil bit to see the logs time.Sleep(3 * time.Second) // kill the process and get the exit status if err := task.Kill(ctx, syscall.SIGTERM); err != nil { return err } // wait for the process to fully exit and print out the exit status status := \u0026lt;-exitStatusC // Block here. code, _, err := status.Result() if err != nil { return err } fmt.Printf(\u0026#34;redis-server exited with status: %d\\n\u0026#34;, code) // For Debug fmt.Printf(\u0026#34;Client: %v\\n\u0026#34;, client) fmt.Printf(\u0026#34;Image: %v\\n\u0026#34;, image) fmt.Printf(\u0026#34;Container: %v\\n\u0026#34;, container) fmt.Printf(\u0026#34;Task: %v\\n\u0026#34;, task) return nil } 「Go 環境の構築」は良い感じにやってください。その状態で上記の Go コードを実行すると、以下のような出力が行われます。\nvagrant@vagrant:~/containerd-playground$ go build main.go \u0026amp;\u0026amp; sudo ./main 1:C 10 Dec 2020 17:11:43.852 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 10 Dec 2020 17:11:43.852 # Redis version=6.0.9, bits=64, commit=00000000, modified=0, pid=1, just started 1:C 10 Dec 2020 17:11:43.852 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 1:M 10 Dec 2020 17:11:43.856 # You requested maxclients of 10000 requiring at least 10032 max file descriptors. 1:M 10 Dec 2020 17:11:43.856 # Server can\u0026#39;t set maximum open files to 10032 because of OS error: Operation not permitted. 1:M 10 Dec 2020 17:11:43.856 # Current maximum open files is 1024. maxclients has been reduced to 992 to compensate for low ulimit. If you need higher maxclients increase \u0026#39;ulimit -n\u0026#39;. 1:M 10 Dec 2020 17:11:43.858 * Running mode=standalone, port=6379. 1:M 10 Dec 2020 17:11:43.858 # Server initialized 1:M 10 Dec 2020 17:11:43.858 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add \u0026#39;vm.overcommit_memory = 1\u0026#39; to /etc/sysctl.conf and then reboot or run the command \u0026#39;sysctl vm.overcommit_memory=1\u0026#39; for this to take effect. 1:M 10 Dec 2020 17:11:43.858 * Ready to accept connections 1:signal-handler (1607620306) Received SIGTERM scheduling shutdown... 1:M 10 Dec 2020 17:11:46.877 # User requested shutdown... 1:M 10 Dec 2020 17:11:46.877 * Saving the final RDB snapshot before exiting. 1:M 10 Dec 2020 17:11:46.879 * DB saved on disk 1:M 10 Dec 2020 17:11:46.879 # Redis is now ready to exit, bye bye... redis-server exited with status: 0 Client: \u0026amp;{{ \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt;} {0 0} 0xc00010ee00 io.containerd.runc.v2 {0xc0002131a0} 0xe6c020} Image: \u0026amp;{0xc0002f4000 {docker.io/library/redis:alpine map[] {application/vnd.docker.distribution.manifest.list.v2+json sha256:b0e84b6b92149194d99953e44f7d1fa1f470a769529bb05b4164eae60d8aea6c 1645 [] map[] \u0026lt;nil\u0026gt;} {312279631 63742833489 \u0026lt;nil\u0026gt;} {749989962 63743217103 \u0026lt;nil\u0026gt;}} {0xc0002131a0}} Container: \u0026amp;{0xc0002f4000 redis-server {redis-server map[] docker.io/library/redis:alpine {io.containerd.runc.v2 \u0026lt;nil\u0026gt;} 0xc0003ce1e0 redis-server-snapshot overlayfs {775955650 63743217103 \u0026lt;nil\u0026gt;} {775955650 63743217103 \u0026lt;nil\u0026gt;} map[]}} Task: \u0026amp;{0xc0002f4000 0xc0001020c0 0xc0003a86c0 redis-server 65235} 上記の Go コードおよび出力について少し説明します。\n上記の Go コードは「containerd と /run/containerd/containerd.sock を通じて通信する client の動作」が記述されています。コード内では client の作成、Container Image の pull、Container Image からの Snapshot (= Container 実行時に root filesystem として利用するもの) および Spec (= Container 実行時のメタデータ、Filesystem Bundle における config.json に相当) の作成、Task (= container の中で実際に実行したいコマンド) の作成、開始、終了およびその待ち合わせを行っています。 また、最後にデバッグ用途に様々な struct を print してみています（これは Getting started に掲載されていたコードに自分が後から追加したものです）。\nPull している Container Image は docker.io/library/redis:alpine で、ログの出力としても redis server の起動および終了が行われていることが分かります。\nこれで、containerd を実際に利用してみる事が出来ました！\nなお、containerd は OSS でコードが公開されているので、気になった挙動はコードを読んで理解する事が出来ます。例えば、 containerd.WithNewSnapshot や containerd.WithNewSpec というのは以下のコードに該当します。\nhttps://github.com/containerd/containerd/blob/v1.4.3/container_opts.go#L144-L169 https://github.com/containerd/containerd/blob/v1.4.3/container_opts.go#L243-L253\n前者のコードを読み進めると「Snapshotter によって Snapshot 作成を行って、Container には SnapshotKey を設定して id を通して参照できるようにしている」事が分かりますし、後者のコードを読み進めると「OCI Runtime Specification の Go binding で定義された Spec struct が generate されて利用される」事が分かります。「runc などの Low-Level Container Runtime が必要とする情報を用意する」という部分をしっかりやってくれてるようです（注: これは自分がコードを読んだ理解なので、誤解してる可能もあります。何か気がついた際はコメントいただけるとありがたいです）。\nその他、Container Image を Pull している部分や Task の管理部分なども興味深い部分です。気になった箇所はぜひ読み進めて理解を深めてみてください。\nまとめ runc や containerd などの Container Runtime を実際に触ってみる事で、理解を深めました。runc については、OCI Runtime Specification で定義された挙動が CLI tool として提供されている事が分かりました。containerd については、Container Image の Pull や Container Image から Filesystem Bundle 相当の情報（= Container 実行に必要な Snapshot および Spec）への変換、Container や Task の作成・削除・開始などの管理を行っている事が分かりました。\nrunc やcontainerd は扱っている領域が異なる一方で、どちらも「Container Runtime」という名前で呼ばれるために混乱してしまいがちです。実際に手を動かしてみる事で、「それぞれの責任範囲」や「レイヤー構造」について理解が深まったように思います。\nなお、このブログ自体は自分の理解のために試したことをまとめたものですが、誰か他の人にとっても理解を助けるものになっていればとても幸いです。\nVagrant での実験環境 以下のような Vagrantfile を使ってます。ubuntu-20.04 を使ってます。\n# -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \u0026#34;2\u0026#34; Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = \u0026#34;bento/ubuntu-20.04\u0026#34; config.vm.synced_folder \u0026#34;./\u0026#34;, \u0026#34;/home/vagrant/oci-playground\u0026#34; end ","permalink":"https://south37.link/posts/20201211-container-3/","summary":"コンテナ標準化の現状と Kubernetes の立ち位置について というブログではコンテナ標準の現状についてまとめてみました。\nまた、手を動かして学ぶコンテナ標準 - Container Image と Container Registry 編 というブログでは Container Image と Container Registry について手を動かして学んでみました。\nこのブログでは、runc, containerd などの Container Runtime について、実際に手を動かして学んでみたいと思います。\nなお、前回のブログ 同様、基本的にこのブログ内のコマンドは Linux で実行するものとします（自分は MacOS で Vagrant で Ubuntu VM を立てて実験してます）。\nrunc を動かしてみる runc は Low-Level Container Runtime と呼ばれるもので、OCI Runtime Specification に準拠した CLI tool となっています。実際に runc を動かしてみることで、Container Runtime に対して理解を深めてみましょう。\nまず runc の install ですが、自分が試している ubuntu-20.04 では apt で install することができます。\nvagrant@vagrant:~$ sudo apt update -y vagrant@vagrant:~$ sudo apt install -y runc vagrant@vagrant:~$ which runc /usr/sbin/runc help を見てみると以下のような内容になっていて、container を操作するために必要な各種 subcommand が存在することがわかります。実は、これらの subcommand によって OCI Runtime Specification の Runtime and Lifecycle で定義された「Operations」 の機能が提供されています。ただし、見比べてみると分かりますが runc 自体はよりリッチな機能を提供しているようです。","title":"手を動かして学ぶコンテナ標準 - Container Runtime 編"},{"content":"先日は、コンテナ標準化の現状と Kubernetes の立ち位置について において、各種ドキュメントをベースにコンテナ標準についてまとめてみました。\nこのブログでは、実際に tool などに触れて手を動かすことで、コンテナ標準についてさらに理解を深めてみたいと思います。\nなお、基本的にこのブログ内のコマンドは、Linux で実行するものとします（自分は MacOS で Vagrant で Ubuntu VM を立てて実験してます）。\nOCI Image の中身を見てみる skopeo と呼ばれる「container image に対して様々な操作を行えるツール」があります。このツールを利用することで、「docker image から OCI Image への変換」を行うことができます。このツールを利用して、実際に OCI Image の中身を見てみましょう。\nまず、以下のコマンドを実行して ruby:2.7.2-slim という docker image を oci:ruby-oci:2.7.2 という名前の OCI Image に変換します。\nvagrant@vagrant:~/oci-playground$ skopeo copy docker://ruby:2.7.2-slim oci:ruby-oci:2.7.2 Getting image source signatures Copying blob 852e50cd189d done Copying blob 6de4319615e2 done Copying blob 150eb06190d1 done Copying blob cf654ff9d9df done Copying blob 0a529f6cf42e done Copying config 3265430f5e done Writing manifest to image destination Storing signatures 上記コマンドを実行すると、ruby-oci という directory が出来ています。\nvagrant@vagrant:~/oci-playground$ ls ruby-oci ruby-oci directory の中を見てみると、以下のように blobs という direcyory と index.json, oci-layout という file が出来ています。これは、OCI Image Format Specification で定められた Image Layout の内容に一致しています。\nvagrant@vagrant:~/oci-playground/ruby-oci$ ls blobs index.json oci-layout oci-layout file には imageLayoutVersion だけが記載されています。現時点では 1.0.0 が記載されているだけなので、将来の拡張のための file と考えると良いでしょう。\nvagrant@vagrant:~/oci-playground/ruby-oci$ cat oci-layout | jq . { \u0026#34;imageLayoutVersion\u0026#34;: \u0026#34;1.0.0\u0026#34; } index.json は OCI Image のエントリーポイントとも呼べる file で、ここには以下のように「manifest fileへの参照（= Image Manifest を指し示す Content Descriptor）」が記載されています。\nvagrant@vagrant:~/oci-playground/ruby-oci$ cat index.json | jq . { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:ad39959791540e6213fbe4675b9f3ee11e96456df3601b0936973ca7ae766bd7\u0026#34;, \u0026#34;size\u0026#34;: 976, \u0026#34;annotations\u0026#34;: { \u0026#34;org.opencontainers.image.ref.name\u0026#34;: \u0026#34;2.7.2\u0026#34; } } ] } ここで出てきた「Content Descriptor」というのが OCI Image Format において特徴的なもので、これは「mediaType, digest, size の 3 つ組 + optional な情報 (e.g. annotations)」となっています。 mediaType が参照先の情報の種類、digest が参照先の情報の path、size が参照先の情報のバイト数を表しています。\ndigest で示されているのは「blobs directory 以下の file path」になっていて、例えば上記の sha256:ad39959791540e6213fbe4675b9f3ee11e96456df3601b0936973ca7ae766bd7 という digest は blobs/sha256/ad39959791540e6213fbe4675b9f3ee11e96456df3601b0936973ca7ae766bd7 という path を表しています。実際に、file の中身を見てみると以下のような JSON になっています。\nvagrant@vagrant:~/oci-playground/ruby-oci$ cat blobs/sha256/ad39959791540e6213fbe4675b9f3ee11e96456df3601b0936973ca7ae766bd7 | jq . { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.config.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:3265430f5e5babe0664d7f7bcc77db2ef7d5feaa1625c06c10b1409ad2952133\u0026#34;, \u0026#34;size\u0026#34;: 4598 }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:852e50cd189dfeb54d97680d9fa6bed21a6d7d18cfb56d6abfe2de9d7f173795\u0026#34;, \u0026#34;size\u0026#34;: 27105484 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:6de4319615e27e1aaaadc89b43db39ea0e118f47eeecfa4c8b910ca2fd810653\u0026#34;, \u0026#34;size\u0026#34;: 12539406 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:150eb06190d1ba56f7b998da25a140c21258bca436d33e2e77df679d77ab364a\u0026#34;, \u0026#34;size\u0026#34;: 198 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:cf654ff9d9df475122683b6bd070fa57a1e1969ced2a45f2c1f76a0678495ef2\u0026#34;, \u0026#34;size\u0026#34;: 22852677 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:0a529f6cf42e0fb49fe3fb4d12e232b26db923ab85a442563b0a7ae0a28c5971\u0026#34;, \u0026#34;size\u0026#34;: 143 } ] } mediaType が application/vnd.oci.image.manifest.v1+json だったことから、これは Image Manifest であると分かります。実際に、Image Manifest の仕様で定義された内容と一致しており、config （Container Image のメタデータ）や layers （Container Image の Layer、Docker Image における Layer Cache の単位となるもの）を情報として持つことも分かります。また、それらの情報への参照も、先ほどと同様の Content Descriptor 形式で表されていることが分かります。\nconfig の内容は、以下のような Image Configuration となっています。環境変数や Command など Container 実行時に必要な各種メタデータや、Container Image 作成時の history の情報が記載されています。\nvagrant@vagrant:~/oci-playground/ruby-oci$ cat blobs/sha256/3265430f5e5babe0664d7f7bcc77db2ef7d5feaa1625c06c10b1409ad2952133 | jq . { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:35:15.373100656Z\u0026#34;, \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;Env\u0026#34;: [ \u0026#34;PATH=/usr/local/bundle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;LANG=C.UTF-8\u0026#34;, \u0026#34;RUBY_MAJOR=2.7\u0026#34;, \u0026#34;RUBY_VERSION=2.7.2\u0026#34;, \u0026#34;RUBY_DOWNLOAD_SHA256=1b95ab193cc8f5b5e59d2686cb3d5dcf1ddf2a86cb6950e0b4bdaae5040ec0d6\u0026#34;, \u0026#34;GEM_HOME=/usr/local/bundle\u0026#34;, \u0026#34;BUNDLE_SILENCE_ROOT_WARNING=1\u0026#34;, \u0026#34;BUNDLE_APP_CONFIG=/usr/local/bundle\u0026#34; ], \u0026#34;Cmd\u0026#34;: [ \u0026#34;irb\u0026#34; ] }, \u0026#34;rootfs\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;diff_ids\u0026#34;: [ \u0026#34;sha256:f5600c6330da7bb112776ba067a32a9c20842d6ecc8ee3289f1a713b644092f8\u0026#34;, \u0026#34;sha256:70ca8ae918406dce7acc5fe0f49e45b9275a266b83e275922e67358976c2929e\u0026#34;, \u0026#34;sha256:e8ace463e6f7085a5439cf3b578a080fbefc8ad8424b59b9f35590adb1509763\u0026#34;, \u0026#34;sha256:71e4ad27368acf7dbb5c90aa65d67cc462267836aa220cbafb9bb62acd9d48de\u0026#34;, \u0026#34;sha256:1946ed62a3cb062940077a7a1dbfc93d55be6ef3d4f605883b42f71970381662\u0026#34; ] }, \u0026#34;history\u0026#34;: [ { \u0026#34;created\u0026#34;: \u0026#34;2020-11-17T20:21:17.570073346Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ADD file:d2abb0e4e7ac1773741f51f57d3a0b8ffc7907348842d773f8c341ba17f856d5 in / \u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-17T20:21:17.865210281Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) CMD [\\\u0026#34;bash\\\u0026#34;]\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:21:22.717162717Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c set -eux; \\tapt-get update; \\tapt-get install -y --no-install-recommends \\t\\tbzip2 \\t\\tca-certificates \\t\\tlibffi-dev \\t\\tlibgmp-dev \\t\\tlibssl-dev \\t\\tlibyaml-dev \\t\\tprocps \\t\\tzlib1g-dev \\t; \\trm -rf /var/lib/apt/lists/*\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:21:23.811888513Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c set -eux; \\tmkdir -p /usr/local/etc; \\t{ \\t\\techo \u0026#39;install: --no-document\u0026#39;; \\t\\techo \u0026#39;update: --no-document\u0026#39;; \\t} \u0026gt;\u0026gt; /usr/local/etc/gemrc\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:21:24.004412503Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENV LANG=C.UTF-8\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:30:41.383881949Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENV RUBY_MAJOR=2.7\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:30:41.629378277Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENV RUBY_VERSION=2.7.2\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:30:41.868222399Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENV RUBY_DOWNLOAD_SHA256=1b95ab193cc8f5b5e59d2686cb3d5dcf1ddf2a86cb6950e0b4bdaae5040ec0d6\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:35:11.770005784Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c set -eux; \\t\\tsavedAptMark=\\\u0026#34;$(apt-mark showmanual)\\\u0026#34;; \\tapt-get update; \\tapt-get install -y --no-install-recommends \\t\\tautoconf \\t\\tbison \\t\\tdpkg-dev \\t\\tgcc \\t\\tlibbz2-dev \\t\\tlibgdbm-compat-dev \\t\\tlibgdbm-dev \\t\\tlibglib2.0-dev \\t\\tlibncurses-dev \\t\\tlibreadline-dev \\t\\tlibxml2-dev \\t\\tlibxslt-dev \\t\\tmake \\t\\truby \\t\\twget \\t\\txz-utils \\t; \\trm -rf /var/lib/apt/lists/*; \\t\\twget -O ruby.tar.xz \\\u0026#34;https://cache.ruby-lang.org/pub/ruby/${RUBY_MAJOR%-rc}/ruby-$RUBY_VERSION.tar.xz\\\u0026#34;; \\techo \\\u0026#34;$RUBY_DOWNLOAD_SHA256 *ruby.tar.xz\\\u0026#34; | sha256sum --check --strict; \\t\\tmkdir -p /usr/src/ruby; \\ttar -xJf ruby.tar.xz -C /usr/src/ruby --strip-components=1; \\trm ruby.tar.xz; \\t\\tcd /usr/src/ruby; \\t\\t{ \\t\\techo \u0026#39;#define ENABLE_PATH_CHECK 0\u0026#39;; \\t\\techo; \\t\\tcat file.c; \\t} \u0026gt; file.c.new; \\tmv file.c.new file.c; \\t\\tautoconf; \\tgnuArch=\\\u0026#34;$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\\\u0026#34;; \\t./configure \\t\\t--build=\\\u0026#34;$gnuArch\\\u0026#34; \\t\\t--disable-install-doc \\t\\t--enable-shared \\t; \\tmake -j \\\u0026#34;$(nproc)\\\u0026#34;; \\tmake install; \\t\\tapt-mark auto \u0026#39;.*\u0026#39; \u0026gt; /dev/null; \\tapt-mark manual $savedAptMark \u0026gt; /dev/null; \\tfind /usr/local -type f -executable -not \\\\( -name \u0026#39;*tkinter*\u0026#39; \\\\) -exec ldd \u0026#39;{}\u0026#39; \u0026#39;;\u0026#39; \\t\\t| awk \u0026#39;/=\u0026gt;/ { print $(NF-1) }\u0026#39; \\t\\t| sort -u \\t\\t| xargs -r dpkg-query --search \\t\\t| cut -d: -f1 \\t\\t| sort -u \\t\\t| xargs -r apt-mark manual \\t; \\tapt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \\t\\tcd /; \\trm -r /usr/src/ruby; \\t! dpkg -l | grep -i ruby; \\t[ \\\u0026#34;$(command -v ruby)\\\u0026#34; = \u0026#39;/usr/local/bin/ruby\u0026#39; ]; \\truby --version; \\tgem --version; \\tbundle --version\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:35:12.227711802Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENV GEM_HOME=/usr/local/bundle\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:35:12.563337139Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENV BUNDLE_SILENCE_ROOT_WARNING=1 BUNDLE_APP_CONFIG=/usr/local/bundle\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:35:12.907595531Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENV PATH=/usr/local/bundle/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:35:14.977063521Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c mkdir -p \\\u0026#34;$GEM_HOME\\\u0026#34; \u0026amp;\u0026amp; chmod 777 \\\u0026#34;$GEM_HOME\\\u0026#34;\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2020-11-18T15:35:15.373100656Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) CMD [\\\u0026#34;irb\\\u0026#34;]\u0026#34;, \u0026#34;empty_layer\u0026#34;: true } ] } layers は Filesystem Layer を表しています。tar+gzip という mediaType の suffix は「gzip 圧縮された tar archive」を表しています。試しに、最も root にあった sha256:852e50cd189dfeb54d97680d9fa6bed21a6d7d18cfb56d6abfe2de9d7f173795 の中身を見てみます。\nvagrant@vagrant:~/oci-playground/ruby-oci$ mkdir rootfs vagrant@vagrant:~/oci-playground/ruby-oci$ tar xvzf blobs/sha256/852e50cd189dfeb54d97680d9fa6bed21a6d7d18cfb56d6abfe2de9d7f173795 -C rootfs/ . . . 上記コマンドで、rootfs directory 以下に圧縮されていた中身が展開されます（注: tar: bin/uncompress: Cannot hard link to ‘bin/gunzip’: Operation not permitted など一部の file について error は出ていて、そのせいで tar: Exiting with failure status due to previous errors という失敗 message も出てしまいましたが、それはここでは無視します）。\nrootfs の中身を見てみると、以下のようにいくつかの directyory が並んでいます。\nvagrant@vagrant:~/oci-playground/ruby-oci$ ls rootfs/ bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 試しに変換前の ruby:2.7.2-slim docker image を利用して container を起動してみると、root directory の中身がそっくりであることが確認できます。\n$ docker run -it ruby:2.7.2-slim bash root@f6be3c7c619d:/# ls / bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 実は、これらの layer が「どう apply されるのか」は Image Layer Filesystem Changeset の Applying Changesets というセクション の中で以下のように明確に定義されています。ざっくり言えば「layer の上から順に tar archive を展開したようなもの」になります。 「file や directory の削除」は Whiteouts と呼ばれる特別な仕様で表現されますが、特別に注意を払う必要があるのはそれくらいのようです。\nApplying Changesets Layer Changesets of media type application/vnd.oci.image.layer.v1.tar are applied, rather than simply extracted as tar archives. Applying a layer changeset requires special consideration for the whiteout files. In the absence of any whiteout files in a layer changeset, the archive is extracted like a regular tar archive. cf. https://github.com/opencontainers/image-spec/blob/v1.0.1/layer.md#applying-changesets\nということで、OCI image の中身に目を通してみました。「Conatainer を走らせるために必要な情報（= 実行時のメタデータ + Layer 化された filesystem の情報）」が格納されてることがわかったかと思います。\nContainer Registry との通信内容を見てみる ここまでで、「Container Image の内容」については把握できました。次に、「Container Registry から Container Image をどのように pull しているのか」を調べてみましょう。\n現在、各種 Container Registry は Docker 社が公開している Docker Registry HTTP API V2 と呼ばれる仕様に従う形で Container Image の Pull を出来るようにしています。実は、「Container Image の Pull」にあたる操作はただの HTTP request であるため、$ curl を利用して実行する事ができます。ここでは、実際に $ curl で request してみることで、Container Registry との通信内容を見てみる事にしましょう。\nなお、自分が試した範囲では、どの Container Image も OCI Image Format ではなく Docker Image Manifest V 2, Schema 2 に従う形の response を返してきました。ただ、OCI Image Format と Docker Image V2.2 は一部の mediaType 名を除いてほぼ同一なので、先ほど眺めた内容は理解に役立つはずです。\nさて、実際に curl で request を送ってみましょう。対象 Container Image は何でも良いのですが、ここでは https://github.com/GoogleContainerTools/base-images-docker に記載されてる Debian の Container Image である gcr.io/google-appengine/debian9 を対象にしてみます。\nまず、以下のように Container Registry の Authentication に必要な Token を取得します。この時、「google-appengine/debian9 の pull」という形で scope を指定しておきます。\n$ export TOKEN=$(curl \u0026#34;https://gcr.io/v2/token?service=gcr.io\u0026amp;scope=repository:google-appengine/debian9:pull\u0026#34; | jq -r \u0026#39;.token\u0026#39;) % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 453 0 453 0 0 1088 0 --:--:-- --:--:-- --:--:-- 1088 次に、https://gcr.io/v2/\u0026lt;name\u0026gt;/manifests/\u0026lt;reference\u0026gt; へ先ほど取得した Token 付きで GET request を送ります。こうすると、Docker Image V2.2 における manifest file が取得できます。\n$ curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://gcr.io/v2/google-appengine/debian9/manifests/latest | jq . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 528 100 528 0 0 469 0 0:00:01 0:00:01 --:--:-- 469 { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.container.image.v1+json\u0026#34;, \u0026#34;size\u0026#34;: 463, \u0026#34;digest\u0026#34;: \u0026#34;sha256:18c47921b263ac67af3d654e3b485c998d1e6bab56edc5a15b6b7a8fad3ac18a\u0026#34; }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.image.rootfs.diff.tar.gzip\u0026#34;, \u0026#34;size\u0026#34;: 47965538, \u0026#34;digest\u0026#34;: \u0026#34;sha256:faa9d9046d25e5fd30ac4444c7b6c30a1a6fff7c461410547156aed2001668a1\u0026#34; } ] } まず、config の中身を見てみましょう。\ndigest を利用して参照を辿る際は https://gcr.io/v2/\u0026lt;name\u0026gt;/blobs/\u0026lt;digest\u0026gt; へ request すれば良いです。実際に request してみると、以下のような response が返ってきます。先ほど OCI Image の中身を見てみた時と同様に、Container 実行に必要なメタデータが格納されていることが分かります。\n$ curl -L -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://gcr.io/v2/google-appengine/debian9/blobs/sha256:18c47921b263ac67af3d654e3b485c998d1e6bab56edc5a15b6b7a8fad3ac18a | jq . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 13 0 13 0 0 23 0 --:--:-- --:--:-- --:--:-- 23 100 463 100 463 0 0 750 0 --:--:-- --:--:-- --:--:-- 750 { \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Bazel\u0026#34;, \u0026#34;created\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;history\u0026#34;: [ { \u0026#34;author\u0026#34;: \u0026#34;Bazel\u0026#34;, \u0026#34;created\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;bazel build ...\u0026#34; } ], \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;rootfs\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;diff_ids\u0026#34;: [ \u0026#34;sha256:0a3dcb016bd8a852985044291de00ad6a6b94dcb0eac01b34b56afed409b9999\u0026#34; ] }, \u0026#34;config\u0026#34;: { \u0026#34;Cmd\u0026#34;: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/bin/bash\u0026#34; ], \u0026#34;Env\u0026#34;: [ \u0026#34;DEBIAN_FRONTEND=noninteractive\u0026#34;, \u0026#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;PORT=8080\u0026#34; ] } } なお、注意点として、どうやら GCR は https://gcr.io/v2/\u0026lt;name\u0026gt;/blobs/\u0026lt;digest\u0026gt; への request では Google Cloud Storage への redirect response を返すようです。-L オプションを付けない場合は以下のような結果になることには留意してください。\n$ curl --include -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://gcr.io/v2/google-appengine/debian9/blobs/sha256:18c47921b263ac67af3d654e3b485c998d1e6bab56edc5a15b6b7a8fad3ac18a HTTP/2 302 docker-distribution-api-version: registry/2.0 location: https://storage.googleapis.com/artifacts.google-appengine.appspot.com/containers/images/sha256:18c47921b263ac67af3d654e3b485c998d1e6bab56edc5a15b6b7a8fad3ac18a content-type: application/json date: Wed, 09 Dec 2020 13:38:51 GMT server: Docker Registry cache-control: private x-xss-protection: 0 x-frame-options: SAMEORIGIN alt-svc: h3-29=\u0026#34;:443\u0026#34;; ma=2592000,h3-T051=\u0026#34;:443\u0026#34;; ma=2592000,h3-Q050=\u0026#34;:443\u0026#34;; ma=2592000,h3-Q046=\u0026#34;:443\u0026#34;; ma=2592000,h3-Q043=\u0026#34;:443\u0026#34;; ma=2592000,quic=\u0026#34;:443\u0026#34;; ma=2592000; v=\u0026#34;46,43\u0026#34; accept-ranges: none vary: Accept-Encoding {\u0026#34;errors\u0026#34;:[]}% 上記では config の取得を行いましたが、Layer （mediaType: application/vnd.docker.image.rootfs.diff.tar.gzip のデータ）についても同様にhttps://gcr.io/v2/\u0026lt;name\u0026gt;/blobs/\u0026lt;digest\u0026gt; への request によって取得する事ができます。先ほどと同様に、tar コマンドで展開すると container 実行に利用される file を取得することが出来ます。\n$ curl -L -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; https://gcr.io/v2/google-appengine/debian9/blobs/sha256:faa9d9046d25e5fd30ac4444c7b6c30a1a6fff7c461410547156aed2001668a1 --output /tmp/faa9d9046d25e5fd30ac4444c7b6c30a1a6fff7c461410547156aed2001668a1 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 13 0 13 0 0 28 0 --:--:-- --:--:-- --:--:-- 28 100 45.7M 100 45.7M 0 0 13.6M 0 0:00:03 0:00:03 --:--:-- 20.1M $ ls -la /tmp/faa9d9046d25e5fd30ac4444c7b6c30a1a6fff7c461410547156aed2001668a1 -rw-r--r-- 1 minami wheel 47965538 Dec 9 23:48 /tmp/faa9d9046d25e5fd30ac4444c7b6c30a1a6fff7c461410547156aed2001668a1 $ mkdir /tmp/rootfs $ tar xvzf /tmp/faa9d9046d25e5fd30ac4444c7b6c30a1a6fff7c461410547156aed2001668a1 -C /tmp/rootfs/ $ ls /tmp/rootfs bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var という事で、Container Registry との通信について、特に「Container Image の Pull」に絞って通信内容を見てみました。Docker Image V2.2 をベースにした通信である事、特に config や layer などがそれぞれの単位で通信できることなどが分かったかと思います。より詳しい内容が気になる場合は、Docker Registry HTTP API V2 を参照してみてください。\nなお、「Container Image 全てをまとめた file を一括でダウンロードしないのは何故なのか？」という疑問についてですが、これは自分の理解では「Layer Cache を効かせた形での Image Pull を実現するため」だと捉えています。Layer のデータは巨大であるため最低限の通信で済ませたいというのが大前提にあり、そのために「コンテンツの中身を反映した digest 値を用いて、Layer ごとに通信する」という振る舞いになっているのだと思われます。\nまとめ OCI image を実際に作成して眺めて見ることで、Container Image について理解を深めました。また、curl で Container Registry との通信を行うことで、Container Registry との通信内容についても理解を深める事が出来ました。\nドキュメントを読むだけだとどうしても理解が曖昧になってしまいがちですが、実際に手を動かす事で具体的な動作をイメージ出来るようになります。このブログ自体は自分の理解のために試したことをまとめたものですが、誰か他の人にとっても理解を助けるものになっていれば幸いです。\nなお、今回は Container Image + Container Registry 編でしたが、後日 Container Runtime についても「手を動かして調べた内容」についてまとめたいと思っています。特に、「Container Image から Container Runtime が利用する Filesystem Bundle への Conversion」や、「runc などの low-level Container Runtime の動作」、「containerd や CRI-O などの high-level Container Runtime の動作」について試したことをまとめる予定です。\n補足: Vagrant での実験環境 以下のような Vagrantfile を使ってます。ubuntu-20.04 を使ってます。\n# -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \u0026#34;2\u0026#34; Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| config.vm.box = \u0026#34;bento/ubuntu-20.04\u0026#34; end 参考文献 以下のブログは、ツールやコマンド、内容において大幅に参考にさせて頂きました。ありがとうございました。\nhttps://blog.unasuke.com/2018/read-oci-image-spec-v101/ https://knqyf263.hatenablog.com/entry/2019/11/29/052818 ","permalink":"https://south37.link/posts/20201210-container-2/","summary":"先日は、コンテナ標準化の現状と Kubernetes の立ち位置について において、各種ドキュメントをベースにコンテナ標準についてまとめてみました。\nこのブログでは、実際に tool などに触れて手を動かすことで、コンテナ標準についてさらに理解を深めてみたいと思います。\nなお、基本的にこのブログ内のコマンドは、Linux で実行するものとします（自分は MacOS で Vagrant で Ubuntu VM を立てて実験してます）。\nOCI Image の中身を見てみる skopeo と呼ばれる「container image に対して様々な操作を行えるツール」があります。このツールを利用することで、「docker image から OCI Image への変換」を行うことができます。このツールを利用して、実際に OCI Image の中身を見てみましょう。\nまず、以下のコマンドを実行して ruby:2.7.2-slim という docker image を oci:ruby-oci:2.7.2 という名前の OCI Image に変換します。\nvagrant@vagrant:~/oci-playground$ skopeo copy docker://ruby:2.7.2-slim oci:ruby-oci:2.7.2 Getting image source signatures Copying blob 852e50cd189d done Copying blob 6de4319615e2 done Copying blob 150eb06190d1 done Copying blob cf654ff9d9df done Copying blob 0a529f6cf42e done Copying config 3265430f5e done Writing manifest to image destination Storing signatures 上記コマンドを実行すると、ruby-oci という directory が出来ています。","title":"手を動かして学ぶコンテナ標準 - Container Image と Container Registry 編"},{"content":"コンテナ標準化が進んでいる事は知りつつも、標準化された仕様の具体的な内容についてはあまり知らない事に気づいたので、この機会に調べてみました。個人向けメモとして残しておきます。\n余力があれば、後でもう少し詳細をまとめる予定です（docker image を OCI Image Format に変換して眺めてみたり、runc や containerd などを実際に動かしてみたり、containerd や CRI-O などの Container Runtime の実装に目を通してみたりしたので、その辺りについてもいつかまとめたいと思ってます）。\n追記: 手を動かして調べた内容は以下の2つのブログにまとめました。\n手を動かして学ぶコンテナ標準 - Container Image と Container Registry 編 手を動かして学ぶコンテナ標準 - Container Runtime 編 以下、調査した内容をまとめたメモです。\nコンテナ標準と Open Container Initiative (OCI) について コンテナ標準は Open Container Initiative (OCI) と呼ばれる団体によって仕様策定が進められている。 image format, (low-level) runtime については既に標準が存在している（v1.0.0 をリリース済み）。distribution (container registry 周り) については仕様策定中の状態（2020年12月6日時点で GitHub の tag では v1.0.0-rc1 が出ている状態）。\n以下、 OCI が定める各種標準仕様について簡単にまとめる。\nOCI Image Format Specification Docker Image のような「container を記述する image format」の標準仕様。これは Docker Image の最新 format である Docker Image Manifest V2, Schema 2 をベースに標準化したもの。 以下が Docker 社からの公式声明。 Given this state of the world in late 2015, the OCI image specification work began in earnest with a strong group of collaborating independent and vendor-associated participants, using the Docker v2.2 image format as a starting point.\ncf. https://opencontainers.org/posts/blog/2018-10-11-oci-image-support-comes-to-open-source-docker-registry/ Container Registry とのデータのやり取りも OCI Image Format（およびそれの元になった Docker Image Manifest V2, Schema2）に準拠する形で行われる。 後述する OCI Runtime Specification で利用される Filesystem Bundle を生成する Conversion 処理 についても仕様が策定されている。 OCI Runtime Specification Container 管理を行う Container Runtime の標準仕様。後述するが、Low-Level Container Runtime と呼ばれるものはこの仕様に準拠している。 これは Docker 社が OCI に寄贈した \u0026ldquo;runC\u0026rdquo; とほぼ対応づくもの。 runc depends on and tracks the runtime-spec repository. We will try to make sure that runc and the OCI specification major versions stay in lockstep. This means that runc 1.0.0 should implement the 1.0 version of the specification.\nhttps://github.com/opencontainers/runc 「コンテナの configuration file およびコンテナの root filesystem をまとめたもの」である Filesystem Bundle や、OCI Runtime 準拠の Container Runtime で行える操作 について仕様を策定している。 OCI Distribution Specification Container Registry との通信における標準仕様。Docker Hub や Google Container Registry など各種 Container Registry が従っている Docker Registry HTTP API V2 をベースに仕様策定が進められている。最新は v1.0.0-rc1。 The spec is based on the specification for the Docker Registry HTTP API V2 protocol apdx-1.\ncf. https://github.com/opencontainers/distribution-spec/blob/master/spec.md Container Runtime について Container Runtime と呼ばれるものは複数存在するが、ものによって担当する layer が違う。以下、一例を紹介（注: 他にも Container Runtime と呼ばれるものはいくつかあるが、ここでは自分の理解のために調べた一部に限定している）。\nrunc 前述した OCI Runtime Specification を素朴に実装したもの。後述する containerd と対比して、Low-Level Container Runtime と紹介をされる例を見かける。 runc is a low-level container runtime, as it directly uses namespace and cgroups to create containers.\nhttps://insujang.github.io/2019-10-31/container-runtime Docker 社が「ツールとしての Docker」の一部を OSS として公開したのが出自。 And today we are spinning out runC as a standalone tool, to be used as plumbing by infrastructure plumbers everywhere.\nhttps://www.docker.com/blog/runc/ OCI Runtime Specification で定められた Filesystem Bundle と呼ばれる「コンテナの configuration file およびコンテナの root filesystem をまとめたもの」を元に、コンテナの作成、削除、状態の取得などの操作が可能。 README の Using runc を参照して動かしてみると動作がイメージできる。 containerd container registry からの image 取得、runc などの low-level container runtime を利用した container 起動（OCI Runtime Specification で定められた filesystem bundle 生成を含む）、container 管理などを行うもの。 daemon として動作する。client からは /run/containerd/containerd.sock 経由の gRPC API で通信を行い、container 作成や task 実行などの操作が可能。 https://containerd.io/docs/getting-started https://github.com/containerd/containerd/blob/master/design/architecture.md 後述する Container Runtime Interface (CRI) もサポートしている 「ツールとしての Docker」から利用されている。そもそも、Docker 社が CNCF へ寄贈したのが出自。 Today, Docker announced its intention to donate the containerd project to the Cloud Native Computing Foundation (CNCF).\nhttps://www.docker.com/blog/docker-donates-containerd-to-cncf/ CRI-O containerd とだいたい似たレイヤーを担当。「Kubernetes から利用されること（CRI に準拠していること）」を念頭に開発されてる。 Last year, the Kubernetes project introduced its Container Runtime Interface (CRI) Building on that work, the CRI-O project (originally known as OCID) is ready to provide a lightweight runtime for Kubernetes.\nhttps://www.redhat.com/en/blog/introducing-cri-o-10 runc を始めとして、いくつかのコンポーネントを組み合わせた実装になっている The plan is to use OCI projects and best of breed libraries for different aspects:\nRuntime: runc (or any OCI runtime-spec implementation) and oci runtime tools Images: Image management using containers/image Storage: Storage and management of image layers using containers/storage Networking: Networking support through use of CNI https://github.com/cri-o/cri-o#what-is-not-in-scope-for-this-project Kubernetes が定める Container Runtime Interface (CRI) について Kubernetes は Container Runtime Interface (CRI) と呼ばれる「独自で定義した API」を利用して、Container Runtime と通信を行う CRI は基本的には「protocol buffer で記述された gRPC API」。container を操作するために必要な操作が RPC として定義されている。 CRI consists of a protocol buffers and gRPC API, and libraries, with additional specifications and tools under active development\ncf. https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/ アクティブに開発されてる Container Runtime は、CRI を実装している。一例は以下（注: Docker については Kubernetes 開発者によって dockershim と呼ばれる CRI サポート用のツールが実装されていた）。 This page lists details for using several common container runtimes with Kubernetes, on Linux:\ncontainerd CRI-O Docker https://kubernetes.io/docs/setup/production-environment/container-runtimes/ 先週、 Docker が Kubernetes の Container Runtime としては Deprecated になった ことが話題になったが、これは dockershim のメンテナンスを将来のバージョンで止めるというアナウンス。 自分が調べた限りでは、containerd, CRI-O などの Container Runtime はどれも複数の image format をサポートしていて、「docker build で生成した container image」は引き続き利用可能。 \u0026ldquo;Docker Image Manifest V2, Schema 2\u0026rdquo; と \u0026ldquo;OCI Image Format Specification\u0026rdquo; はどちらもサポートされている。 そもそもこれらは「ほぼ同一」ある（一部の mediaType が違うくらいで、1対1対応） The OCI image manifest is also a registry image manifest that defines components that make up an image. The format is essentially the same as the Docker V2.2 format, with a few differences.\nmediaType - must be set to application/vnd.oci.image.manifest.v1+json config.mediaType - must be set to application/vnd.oci.image.config.v1+json Each object in layers must have mediaType be either application/vnd.oci.image.layer.v1.tar+gzip or application/vnd.oci.image.layer.v1.tar.\nhttps://containers.gitbook.io/build-containers-the-hard-way/#registry-format-oci-image-manifest そのため、「docker build で作成した container image を container registry へ push して、k8s から利用する」という一連のワークフローに関していえば、containerd or CRI-O を使っても問題になる事は一切無い はず もちろん、以下の Blog で言及されてるように「Kuberrnetes 内で Docker 自体の機能に依存していた場合」は対応が必要 cf. KubernetesのDockershim廃止における開発者の対応 ","permalink":"https://south37.link/posts/20201207-container-1/","summary":"コンテナ標準化が進んでいる事は知りつつも、標準化された仕様の具体的な内容についてはあまり知らない事に気づいたので、この機会に調べてみました。個人向けメモとして残しておきます。\n余力があれば、後でもう少し詳細をまとめる予定です（docker image を OCI Image Format に変換して眺めてみたり、runc や containerd などを実際に動かしてみたり、containerd や CRI-O などの Container Runtime の実装に目を通してみたりしたので、その辺りについてもいつかまとめたいと思ってます）。\n追記: 手を動かして調べた内容は以下の2つのブログにまとめました。\n手を動かして学ぶコンテナ標準 - Container Image と Container Registry 編 手を動かして学ぶコンテナ標準 - Container Runtime 編 以下、調査した内容をまとめたメモです。\nコンテナ標準と Open Container Initiative (OCI) について コンテナ標準は Open Container Initiative (OCI) と呼ばれる団体によって仕様策定が進められている。 image format, (low-level) runtime については既に標準が存在している（v1.0.0 をリリース済み）。distribution (container registry 周り) については仕様策定中の状態（2020年12月6日時点で GitHub の tag では v1.0.0-rc1 が出ている状態）。\n以下、 OCI が定める各種標準仕様について簡単にまとめる。\nOCI Image Format Specification Docker Image のような「container を記述する image format」の標準仕様。これは Docker Image の最新 format である Docker Image Manifest V2, Schema 2 をベースに標準化したもの。 以下が Docker 社からの公式声明。 Given this state of the world in late 2015, the OCI image specification work began in earnest with a strong group of collaborating independent and vendor-associated participants, using the Docker v2.","title":"コンテナ標準化の現状と Kubernetes との関係性について"}]